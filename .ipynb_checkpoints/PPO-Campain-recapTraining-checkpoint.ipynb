{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e76814-2083-496f-9cba-835f02a7fa47",
   "metadata": {},
   "source": [
    "# PPO- based Pysc2\n",
    "\n",
    "Modular Design\n",
    "sc2_ppo_project/\n",
    "\n",
    "├── main.py             # Entry-point to run training\n",
    "\n",
    "├── config.py           # Hyperparameters and logging config\n",
    "\n",
    "├── environment.py      # SC2 environment wrapper\n",
    "\n",
    "├── model.py            # Actor-Critic neural network\n",
    "\n",
    "├── utils.py            # Observation preprocessing and action utilities\n",
    "\n",
    "└── ppo.py              # PPO training algorithm implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f92e15-9658-4ec9-8e6c-2b6e7769257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9344929d-431d-4821-8768-35f16cb59ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ─── Hyperparameters ─────────────────────────────\n",
    "MAP_NAME     = \"DefeatZerglingsAndBanelings\"\n",
    "# MAP_NAME     = \"Simple64\" \n",
    "\n",
    "SCREEN_SIZE  = 84\n",
    "MINIMAP_SIZE = 64\n",
    "STEP_MUL     = 16\n",
    "NB_ACTORS    = 1\n",
    "T            = 128\n",
    "K            = 10\n",
    "BATCH_SIZE   = 256\n",
    "GAMMA        = 0.99\n",
    "GAE_LAMBDA   = 0.95\n",
    "LR           = 2.5e-4\n",
    "ENT_COEF     = 0.01\n",
    "VF_COEF      = 1.0\n",
    "MAX_ITERS    = 1000\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Replay and Dataset Directories ─────────────\n",
    "REPLAY_DIR   = os.path.join(\"replays\")             # Where to save .SC2Replay files\n",
    "DATASET_PATH = os.path.join(\"dataset.pkl\")         # Where to save (obs, action) dataset\n",
    "\n",
    "# ─── Logging Configuration ──────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure replay directory exists\n",
    "os.makedirs(REPLAY_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c30bd3-1eff-4263-93d8-d42fc320ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "656af9d7-6973-448b-b4f6-a6ca44b13d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysc2.env import sc2_env\n",
    "# from pysc2.lib import actions, features\n",
    "# # from config import MAP_NAME, SCREEN_SIZE, MINIMAP_SIZE, STEP_MUL, logger\n",
    "\n",
    "# class SC2Envs:\n",
    "#     def __init__(self, nb_actor):\n",
    "#         logger.info(\"Initializing %d SC2 env(s)...\", nb_actor)\n",
    "#         self.nb   = nb_actor\n",
    "#         self.envs = [self._make_env() for _ in range(nb_actor)]\n",
    "#         self.obs  = [None]*nb_actor\n",
    "#         self.done = [False]*nb_actor\n",
    "#         self._init_all()\n",
    "#         logger.info(\"All SC2 env(s) ready.\")\n",
    "\n",
    "#     def _make_env(self):\n",
    "#         return sc2_env.SC2Env(\n",
    "#             map_name=MAP_NAME,\n",
    "#             players=[sc2_env.Agent(sc2_env.Race.terran)],\n",
    "#             agent_interface_format=features.AgentInterfaceFormat(\n",
    "#                 feature_dimensions=features.Dimensions(\n",
    "#                     screen=SCREEN_SIZE, minimap=MINIMAP_SIZE),\n",
    "#                 use_feature_units=True,\n",
    "#                 use_raw_units=False,\n",
    "#                 use_camera_position=True,\n",
    "#                 action_space=actions.ActionSpace.FEATURES\n",
    "#             ),\n",
    "#             step_mul=STEP_MUL,\n",
    "#             game_steps_per_episode=0,\n",
    "#             visualize=False,\n",
    "#         )\n",
    "\n",
    "#     def _init_all(self):\n",
    "#         for i, e in enumerate(self.envs):\n",
    "#             ts = e.reset()[0]\n",
    "#             self.obs[i], self.done[i] = ts, False\n",
    "\n",
    "#     def reset(self, i):\n",
    "#         ts = self.envs[i].reset()[0]\n",
    "#         self.obs[i], self.done[i] = ts, False\n",
    "#         return ts\n",
    "\n",
    "#     def step(self, i, fc):\n",
    "#         ts = self.envs[i].step([fc])[0]\n",
    "#         self.obs[i], self.done[i] = ts, ts.last()\n",
    "#         return ts\n",
    "\n",
    "#     def close(self):\n",
    "#         for e in self.envs:\n",
    "#             e.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "009ecfc0-4629-46d1-9c03-55c6733f10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SC2EnvsMulti:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_actor,\n",
    "        replay_dir=\"replays\",\n",
    "        replay_prefix=\"run\",\n",
    "        map_name=MAP_NAME,\n",
    "        screen_size=84,\n",
    "        minimap_size=64,\n",
    "        step_mul=16,\n",
    "    ):\n",
    "        self.nb = nb_actor\n",
    "\n",
    "        # — make replay_dir an absolute folder INSIDE YOUR PROJECT\n",
    "        self.replay_dir = os.path.join(os.getcwd(), replay_dir)\n",
    "        os.makedirs(self.replay_dir, exist_ok=True)\n",
    "\n",
    "        self.replay_prefix = replay_prefix\n",
    "\n",
    "        logger.info(\"Initializing %d SC2 env(s)…\", self.nb)\n",
    "        self.envs = [\n",
    "            self._make_env(map_name, screen_size, minimap_size, step_mul)\n",
    "            for _ in range(self.nb)\n",
    "        ]\n",
    "        self.obs  = [env.reset()[0] for env in self.envs]\n",
    "        self.done = [False] * self.nb\n",
    "\n",
    "    def _make_env(self, map_name, screen_size, minimap_size, step_mul):\n",
    "        return sc2_env.SC2Env(\n",
    "            map_name=map_name,\n",
    "            players=[\n",
    "                sc2_env.Agent(sc2_env.Race.terran),\n",
    "                sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty.very_easy),\n",
    "            ],\n",
    "            agent_interface_format=features.AgentInterfaceFormat(\n",
    "                feature_dimensions=features.Dimensions(\n",
    "                    screen=screen_size, minimap=minimap_size\n",
    "                ),\n",
    "                use_feature_units=True,\n",
    "            ),\n",
    "            step_mul=step_mul,\n",
    "            visualize=False,\n",
    "\n",
    "            # ← now uses the absolute project path\n",
    "            save_replay_episodes=1,\n",
    "            replay_dir=self.replay_dir,\n",
    "            replay_prefix=self.replay_prefix,\n",
    "        )\n",
    "\n",
    "    def step(self, i, action):\n",
    "        ts = self.envs[i].step([action])[0]\n",
    "        self.obs[i]  = ts\n",
    "        self.done[i] = ts.last()\n",
    "        return ts\n",
    "\n",
    "    def reset(self, i):\n",
    "        self.obs[i]  = self.envs[i].reset()[0]\n",
    "        self.done[i] = False\n",
    "\n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f776099f-7adf-49ec-b782-9b7d1f38d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d359419f-72a0-4747-aeec-d3cdb502bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from config import SCREEN_SIZE, DEVICE  # ✅ Import shared config\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, in_channels, nb_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 8, stride=4), nn.Tanh(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2), nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, SCREEN_SIZE, SCREEN_SIZE).to(DEVICE)\n",
    "            conv_out = self.conv(dummy).shape[-1]\n",
    "\n",
    "        self.fc     = nn.Sequential(nn.Linear(conv_out, 256), nn.Tanh())\n",
    "        self.actor  = nn.Linear(256, nb_actions)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = self.fc(h)\n",
    "        return self.actor(h), self.critic(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c70587f2-7223-4547-afe0-6a9f1c26f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28143d60-a430-4f01-829a-f1278b94f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from pysc2.lib import actions, features\n",
    "# # from config import DEVICE\n",
    "\n",
    "# _PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "# _UNIT_TYPE       = features.SCREEN_FEATURES.unit_type.index\n",
    "\n",
    "# ACTION_LIST = ['do_nothing', 'select_idle', 'build_refinery', 'harvest']\n",
    "# FUNC_ID = {\n",
    "#     'do_nothing': actions.FUNCTIONS.no_op.id,\n",
    "#     'select_idle': actions.FUNCTIONS.select_idle_worker.id,\n",
    "#     'build_refinery': actions.FUNCTIONS.Build_Refinery_screen.id,\n",
    "#     'harvest': actions.FUNCTIONS.Harvest_Gather_screen.id,\n",
    "# }\n",
    "\n",
    "# def preprocess(ts):\n",
    "#     fs = ts.observation.feature_screen\n",
    "#     pr = fs[_PLAYER_RELATIVE].astype(np.float32) / 4.0\n",
    "#     ut = fs[_UNIT_TYPE].astype(np.float32) / fs[_UNIT_TYPE].max()\n",
    "#     stacked = np.stack([pr, ut], axis=0)\n",
    "#     return torch.from_numpy(stacked).unsqueeze(0).float().to(DEVICE)\n",
    "\n",
    "# def legal_actions(ts):\n",
    "#     avail = set(ts.observation.available_actions)\n",
    "#     fus   = ts.observation.feature_units\n",
    "#     legal = [0]\n",
    "#     if FUNC_ID['select_idle'] in avail: legal.append(1)\n",
    "#     if FUNC_ID['build_refinery'] in avail and any(u.unit_type==342 for u in fus): legal.append(2)\n",
    "#     if FUNC_ID['harvest'] in avail and any(u.unit_type==341 for u in fus): legal.append(3)\n",
    "#     return legal\n",
    "\n",
    "# def make_pysc2_call(action_idx, ts):\n",
    "#     name, fid = ACTION_LIST[action_idx], FUNC_ID[ACTION_LIST[action_idx]]\n",
    "#     if name == 'select_idle':\n",
    "#         return actions.FunctionCall(fid, [[2]])\n",
    "#     if name in ('build_refinery','harvest'):\n",
    "#         fus = ts.observation.feature_units\n",
    "#         cand = [u for u in fus if (u.unit_type==342 if name=='build_refinery' else u.unit_type==341)]\n",
    "#         if not cand:\n",
    "#             return actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "#         u = random.choice(cand)\n",
    "#         return actions.FunctionCall(fid, [[0],[u.x,u.y]])\n",
    "#     return actions.FunctionCall(fid, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9259464-83a6-4c0d-a9a9-7b536dd1af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from pysc2.lib import actions, features\n",
    "\n",
    "# ─── Constants ────────────────────────────────────────────────────────────────\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE       = features.SCREEN_FEATURES.unit_type.index\n",
    "\n",
    "ACTION_LIST  = ['select', 'do_nothing','build', 'gather', 'move', 'attack','train', 'upgrade']\n",
    "ACTION_INDEX = {name: idx for idx, name in enumerate(ACTION_LIST)}\n",
    "\n",
    "SCREEN_SIZE = 84\n",
    "\n",
    "TERRAN_STRUCTURE_TYPES = [\n",
    "    18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30,\n",
    "    130, 131, 132, 133\n",
    "]\n",
    "\n",
    "# ─── Observation Preprocessing ───────────────────────────────────────────────\n",
    "def safe_coords(x, y, screen_size=SCREEN_SIZE):\n",
    "    x = max(0, min(screen_size - 1, x))\n",
    "    y = max(0, min(screen_size - 1, y))\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def preprocess(ts):\n",
    "    fs = ts.observation.feature_screen\n",
    "    pr = fs[_PLAYER_RELATIVE].astype(np.float32) / 4.0\n",
    "    ut = fs[_UNIT_TYPE].astype(np.float32) / fs[_UNIT_TYPE].max()\n",
    "    stacked = np.stack([pr, ut], axis=0)\n",
    "    return torch.from_numpy(stacked).unsqueeze(0).float()\n",
    "\n",
    "def legal_actions(ts):\n",
    "    avail = set(ts.observation.available_actions)\n",
    "    fus   = ts.observation.feature_units\n",
    "\n",
    "    legal = []\n",
    "\n",
    "    # 1) Always allow selecting one of your own units\n",
    "    if actions.FUNCTIONS.select_point.id in avail:\n",
    "        legal.append(ACTION_INDEX['select'])\n",
    "\n",
    "    # 2) Fallback to no_op if nothing else\n",
    "    legal.append(ACTION_INDEX['do_nothing'])\n",
    "\n",
    "    # 3) Movement & combat\n",
    "    if actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        legal.append(ACTION_INDEX['move'])\n",
    "    if actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        legal.append(ACTION_INDEX['attack'])\n",
    "\n",
    "    # 4) Building (non-quick variants only)\n",
    "    build_opts = [\n",
    "        a for a in avail\n",
    "        if 'Build' in actions.FUNCTIONS[a].name\n",
    "        and not actions.FUNCTIONS[a].name.endswith('_quick')\n",
    "    ]\n",
    "    if build_opts:\n",
    "        legal.append(ACTION_INDEX['build'])\n",
    "\n",
    "    # 5) Gathering\n",
    "    if (actions.FUNCTIONS.Harvest_Gather_screen.id in avail and\n",
    "        any(u.unit_type == 341 for u in fus)):\n",
    "        legal.append(ACTION_INDEX['gather'])\n",
    "\n",
    "    # 6) Tech upgrades\n",
    "    if any('Research' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['upgrade'])\n",
    "\n",
    "    # 7) Training units\n",
    "    if any('Train' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['train'])\n",
    "\n",
    "    return legal\n",
    "\n",
    "\n",
    "\n",
    "# ─── PySC2 Action Execution Wrapper ─────────────────────────────────────────\n",
    "# ─── PySC2 Action Execution Wrapper ─────────────────────────────────────────\n",
    "def make_pysc2_call(action_idx, ts, pending=None):\n",
    "    \"\"\"\n",
    "    Returns (FunctionCall, new_pending).\n",
    "    Pads missing argument lists, clamps all coordinates, and falls back to\n",
    "    the best available selection primitive.\n",
    "    \"\"\"\n",
    "    def _pad(fn_id, args):\n",
    "        fn = actions.FUNCTIONS[fn_id]\n",
    "        missing = len(fn.args) - len(args)\n",
    "        if missing > 0:\n",
    "            args = args + [[0]] * missing\n",
    "        return args\n",
    "\n",
    "    obs   = ts.observation\n",
    "    fus   = obs.feature_units\n",
    "    avail = set(obs.available_actions)\n",
    "\n",
    "    # 1) Fire pending follow-up\n",
    "    if pending:\n",
    "        fn_id = pending['action_fn']\n",
    "        if fn_id in avail:\n",
    "            args = _pad(fn_id, pending['args'])\n",
    "            return actions.FunctionCall(fn_id, args), None\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    # 2) Handle explicit no_op\n",
    "    if action_idx == ACTION_INDEX['do_nothing']:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    # 3) Pick best select primitive\n",
    "    if actions.FUNCTIONS.select_point.id in avail:\n",
    "        select_fn = actions.FUNCTIONS.select_point.id\n",
    "    elif actions.FUNCTIONS.select_unit.id in avail:\n",
    "        select_fn = actions.FUNCTIONS.select_unit.id\n",
    "    elif actions.FUNCTIONS.select_rect.id in avail:\n",
    "        select_fn = actions.FUNCTIONS.select_rect.id\n",
    "    else:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    # choose a random friendly unit\n",
    "    self_units = [u for u in fus if u.alliance == features.PlayerRelative.SELF]\n",
    "    if not self_units:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    unit   = random.choice(self_units)\n",
    "    x, y   = safe_coords(unit.x, unit.y)\n",
    "    select = actions.FunctionCall(\n",
    "        select_fn,\n",
    "        _pad(select_fn, [[0], [x, y]])\n",
    "    )\n",
    "\n",
    "    # 4) Prepare any follow-up action (with clamped targets)\n",
    "    pending = None\n",
    "    if action_idx == ACTION_INDEX['train']:\n",
    "        opts = [a for a in avail if 'Train' in actions.FUNCTIONS[a].name]\n",
    "        if opts:\n",
    "            pending = {'action_fn': random.choice(opts), 'args': [[0]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['move'] and actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        fn_id = actions.FUNCTIONS.Move_screen.id\n",
    "        tx, ty = safe_coords(\n",
    "            random.randrange(0, SCREEN_SIZE),\n",
    "            random.randrange(0, SCREEN_SIZE)\n",
    "        )\n",
    "        pending = {'action_fn': fn_id, 'args': [[0], [tx, ty]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['attack'] and actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        enemies = [u for u in fus if u.alliance == features.PlayerRelative.ENEMY]\n",
    "        if enemies:\n",
    "            fn_id = actions.FUNCTIONS.Attack_screen.id\n",
    "            tx, ty = safe_coords(enemies[0].x, enemies[0].y)\n",
    "            pending = {'action_fn': fn_id, 'args': [[0], [tx, ty]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['gather'] and actions.FUNCTIONS.Harvest_Gather_screen.id in avail:\n",
    "        minerals = [u for u in fus if u.unit_type == 341]\n",
    "        if minerals:\n",
    "            fn_id = actions.FUNCTIONS.Harvest_Gather_screen.id\n",
    "            tx, ty = safe_coords(minerals[0].x, minerals[0].y)\n",
    "            pending = {'action_fn': fn_id, 'args': [[0], [tx, ty]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['build']:\n",
    "        opts = [\n",
    "            a for a in avail\n",
    "            if 'Build' in actions.FUNCTIONS[a].name\n",
    "            and not actions.FUNCTIONS[a].name.endswith('_quick')\n",
    "        ]\n",
    "        if opts:\n",
    "            fn_id     = random.choice(opts)\n",
    "            buildable = np.argwhere(obs.feature_screen.buildable == 1)\n",
    "            if buildable.size:\n",
    "                by, bx = random.choice(buildable)\n",
    "                bx, by = safe_coords(bx, by)\n",
    "                pending = {'action_fn': fn_id, 'args': [[0], [bx, by]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['upgrade']:\n",
    "        opts = [a for a in avail if 'Research' in actions.FUNCTIONS[a].name]\n",
    "        if opts:\n",
    "            pending = {'action_fn': random.choice(opts), 'args': [[0]]}\n",
    "\n",
    "    return select, pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95d4b4d6-c7fb-47f8-9b29-89ca20299c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO training LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f710a40-241f-4cef-91a8-4413a650d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from pysc2.lib import actions\n",
    "from config import *\n",
    "# from utils import preprocess, legal_actions, make_pysc2_call, extract_enemy_units, infer_enemy_action\n",
    "\n",
    "def PPO(envs, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1.0, end_factor=0.0, total_iters=MAX_ITERS\n",
    "    )\n",
    "\n",
    "    ep_rewards = []\n",
    "    expert_dataset = []  # Collect enemy bot data\n",
    "\n",
    "    logger.info(\"▶️  Starting PPO for %d iterations\", MAX_ITERS)\n",
    "    for it in range(MAX_ITERS):\n",
    "        if it % 1000 == 0:\n",
    "            logger.info(\"🔄 Iter %d / %d\", it, MAX_ITERS)\n",
    "\n",
    "        # storage buffers\n",
    "        obs_buf  = torch.zeros(envs.nb, T, 2, SCREEN_SIZE, SCREEN_SIZE, device=DEVICE)\n",
    "        act_buf  = torch.zeros(envs.nb, T,      dtype=torch.long, device=DEVICE)\n",
    "        logp_buf = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        val_buf  = torch.zeros(envs.nb, T+1,                   device=DEVICE)\n",
    "        rew_buf  = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        done_buf = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        adv_buf  = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "\n",
    "        # ─── Rollout ─────────────────────────────────────────────────────────\n",
    "        with torch.no_grad():\n",
    "            for t in range(T):\n",
    "                for i in range(envs.nb):\n",
    "                    ts    = envs.obs[i]\n",
    "                    state = preprocess(ts)\n",
    "                    logits, value = model(state)\n",
    "\n",
    "                    # mask illegal\n",
    "                    LA   = legal_actions(ts)\n",
    "                    mask = torch.full_like(logits, float('-inf'))\n",
    "                    mask[0, LA] = 0.0\n",
    "                    dist = Categorical(logits=logits + mask)\n",
    "\n",
    "                    action = dist.sample()\n",
    "                    logp   = dist.log_prob(action)\n",
    "                    fc     = make_pysc2_call(action.item(), ts)\n",
    "\n",
    "                    # step (fallback to no-op)\n",
    "                    try:\n",
    "                        ts2 = envs.step(i, fc)\n",
    "                    except ValueError:\n",
    "                        ts2 = envs.step(i, actions.FunctionCall(actions.FUNCTIONS.no_op.id, []))\n",
    "\n",
    "                    r = ts2.reward\n",
    "                    d = float(ts2.last())\n",
    "\n",
    "                    obs_buf[i,t]  = state\n",
    "                    act_buf[i,t]  = action\n",
    "                    logp_buf[i,t] = logp\n",
    "                    val_buf[i,t]  = value\n",
    "                    rew_buf[i,t]  = r\n",
    "                    done_buf[i,t] = d\n",
    "\n",
    "                    if d:\n",
    "                        ep_rewards.append(sum(rew_buf[i, :t+1].tolist()))\n",
    "                        envs.reset(i)\n",
    "\n",
    "            for i in range(envs.nb):\n",
    "                val_buf[i,T] = model(preprocess(envs.obs[i]))[1]\n",
    "\n",
    "        # ─── GAE & flatten ────────────────────────────────────────────────────\n",
    "        for i in range(envs.nb):\n",
    "            gae = 0\n",
    "            for t in reversed(range(T)):\n",
    "                mask  = 1.0 - done_buf[i,t]\n",
    "                delta = rew_buf[i,t] + GAMMA*val_buf[i,t+1]*mask - val_buf[i,t]\n",
    "                gae   = delta + GAMMA*GAE_LAMBDA*mask*gae\n",
    "                adv_buf[i,t] = gae\n",
    "\n",
    "        b_s  = obs_buf.reshape(-1,2,SCREEN_SIZE,SCREEN_SIZE)\n",
    "        b_a  = act_buf.reshape(-1)\n",
    "        b_lp = logp_buf.reshape(-1)\n",
    "        b_v  = val_buf[:,:T].reshape(-1)\n",
    "        b_ad = adv_buf.reshape(-1)\n",
    "\n",
    "        # ─── PPO updates ─────────────────────────────────────────────────────\n",
    "        for _ in range(K):\n",
    "            ds     = TensorDataset(b_s,b_a,b_lp,b_v,b_ad)\n",
    "            loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            for st, ac, old_lp, old_v, adv in loader:\n",
    "                logits, val = model(st)\n",
    "                dist        = Categorical(logits=logits)\n",
    "                lp          = dist.log_prob(ac)\n",
    "                ratio       = torch.exp(lp - old_lp)\n",
    "\n",
    "                clip   = 0.1 * (1 - it/MAX_ITERS)\n",
    "                obj1   = adv * ratio\n",
    "                obj2   = adv * torch.clamp(ratio, 1-clip, 1+clip)\n",
    "                p_loss = -torch.min(obj1,obj2).mean()\n",
    "\n",
    "                ret     = adv + old_v\n",
    "                v1      = (val - ret).pow(2)\n",
    "                v2      = (torch.clamp(val,old_v-clip,old_v+clip)-ret).pow(2)\n",
    "                v_loss  = 0.5 * torch.max(v1,v2).mean()\n",
    "\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss    = p_loss + VF_COEF*v_loss - ENT_COEF*entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # ─── Save replays only at end ────────────────────────────────────────────\n",
    "    for i in range(envs.nb):\n",
    "        replay_path = os.path.join(REPLAY_DIR, f\"ppo_final_{i}.SC2Replay\")\n",
    "        envs.envs[i]._save_replay(\"PPO\", replay_path)\n",
    "    logger.info(\"💾 Saved final replay(s) to %s\", REPLAY_DIR)\n",
    "\n",
    "    # ─── Plot learning curve ─────────────────────────────────────────────────\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(ep_rewards, label=\"episode reward\")\n",
    "    plt.title(\"Environment Reward per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    envs.close()\n",
    "    logger.info(\"✅ Training complete\")\n",
    "    logger.info(f\"Saved learning_curve.png over {len(ep_rewards)} episodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdcecead-f59c-4bbd-827d-771ad655c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47d18d4c-cd99-477a-be83-74fd6005978b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from absl import app\n",
    "# # from environment import SC2Envs\n",
    "# # from model import ActorCritic\n",
    "# # from ppo import PPO\n",
    "# # from config import NB_ACTORS, DEVICE\n",
    "# # from utils import ACTION_LIST\n",
    "\n",
    "# def main(_):\n",
    "#     envs = SC2Envs(NB_ACTORS)\n",
    "#     model = ActorCritic(2, len(ACTION_LIST)).to(DEVICE)\n",
    "#     PPO(envs, model)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#     sys.argv = sys.argv[:1]  # Remove extra flags passed by Jupyter or IPython\n",
    "#     app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "942d6802-c091-4439-9f6c-a28b16ccbb30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from rich.live import Live\n",
    "# from rich.table import Table\n",
    "# from rich.console import Console\n",
    "# from collections import deque\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import sys\n",
    "# from absl import flags\n",
    "\n",
    "# flags.FLAGS(sys.argv)  # fix required by pysc2\n",
    "# # from util import preprocess, legal_actions, make_pysc2_call\n",
    "# # from env import SC2Envs\n",
    "\n",
    "# console = Console()\n",
    "# envs = SC2Envs(nb_actor=1)\n",
    "# pending_action = [None] * envs.nb\n",
    "\n",
    "# MAX_ROWS = 20\n",
    "# recent_rows = deque(maxlen=MAX_ROWS)\n",
    "\n",
    "# # For tracking per-episode scores\n",
    "# episode_score = [0] * envs.nb\n",
    "# scores = []\n",
    "\n",
    "# def generate_table():\n",
    "#     table = Table(title=f\"SC2 Agent Actions (Last {MAX_ROWS} Steps)\", expand=True)\n",
    "#     table.add_column(\"Step\", justify=\"right\")\n",
    "#     table.add_column(\"Function ID\", justify=\"right\")\n",
    "#     table.add_column(\"Args\", justify=\"left\")\n",
    "#     for row in recent_rows:\n",
    "#         table.add_row(*row)\n",
    "#     return table\n",
    "\n",
    "# with Live(generate_table(), refresh_per_second=10, console=console, transient=True) as live:\n",
    "#     for step in range(MAX_ITERS):\n",
    "#         for i in range(envs.nb):\n",
    "#             ts = envs.obs[i]\n",
    "\n",
    "#             if pending_action[i]:\n",
    "#                 action, pending_action[i] = make_pysc2_call(None, ts, pending_action[i])\n",
    "#             else:\n",
    "#                 legal = legal_actions(ts)\n",
    "#                 action_idx = random.choice(legal)\n",
    "#                 action, pending_action[i] = make_pysc2_call(action_idx, ts)\n",
    "\n",
    "#             recent_rows.append((str(step), str(action.function), str(action.arguments)))\n",
    "#             live.update(generate_table())\n",
    "\n",
    "#             ts = envs.step(i, action)\n",
    "#             episode_score[i] += ts.reward\n",
    "\n",
    "#             if ts.last():\n",
    "#                 scores.append(episode_score[i])\n",
    "#                 episode_score[i] = 0  # reset\n",
    "#                 envs.reset(i)\n",
    "\n",
    "# envs.close()\n",
    "\n",
    "# # Plot episode scores\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(scores, label=\"Episode Score\", marker='o', linewidth=1.5)\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Total Score\")\n",
    "# plt.title(\"Agent Score per Episode\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a1044ec-ea4c-4ed4-91c4-6cede1bb862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Campain Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e3b570e-8f16-4ebf-ba44-aef48488f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "from pysc2.lib import actions\n",
    "# from config import (\n",
    "#     NB_ACTORS, T, K, BATCH_SIZE,\n",
    "#     GAMMA, GAE_LAMBDA, LR, ENT_COEF, VF_COEF,\n",
    "#     MAX_ITERS, DEVICE, SCREEN_SIZE, REPLAY_DIR\n",
    "# )\n",
    "# from utils import preprocess, legal_actions, make_pysc2_call\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train_PPO(envs, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1.0, end_factor=0.0, total_iters=MAX_ITERS\n",
    "    )\n",
    "\n",
    "    episode_rewards = []\n",
    "\n",
    "    logger.info(\"▶️  Starting PPO for %d iterations\", MAX_ITERS)\n",
    "    for it in range(1, MAX_ITERS + 1):\n",
    "        # storage buffers\n",
    "        obs_buf  = torch.zeros(NB_ACTORS, T, 2, SCREEN_SIZE, SCREEN_SIZE, device=DEVICE)\n",
    "        act_buf  = torch.zeros(NB_ACTORS, T,      dtype=torch.long, device=DEVICE)\n",
    "        logp_buf = torch.zeros(NB_ACTORS, T,                     device=DEVICE)\n",
    "        val_buf  = torch.zeros(NB_ACTORS, T+1,                   device=DEVICE)\n",
    "        rew_buf  = torch.zeros(NB_ACTORS, T,                     device=DEVICE)\n",
    "        done_buf = torch.zeros(NB_ACTORS, T,                     device=DEVICE)\n",
    "        adv_buf  = torch.zeros(NB_ACTORS, T,                     device=DEVICE)\n",
    "\n",
    "        # ─── Rollout ─────────────────────────────────────────────────────────────\n",
    "        with torch.no_grad():\n",
    "            for t in range(T):\n",
    "                for i in range(NB_ACTORS):\n",
    "                    ts = envs.obs[i]\n",
    "                    state = preprocess(ts)\n",
    "                    logits, value = model(state)\n",
    "\n",
    "                    # mask illegal actions\n",
    "                    legal = legal_actions(ts)\n",
    "                    mask = torch.full_like(logits, float('-inf'))\n",
    "                    mask[0, legal] = 0\n",
    "                    dist = Categorical(logits=logits + mask)\n",
    "\n",
    "                    action = dist.sample()\n",
    "                    logp   = dist.log_prob(action)\n",
    "\n",
    "                    fn_call, _ = make_pysc2_call(action.item(), ts)\n",
    "                    try:\n",
    "                        ts2 = envs.step(i, fn_call)\n",
    "                    except ValueError:\n",
    "                        ts2 = envs.step(i,\n",
    "                            actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "                        )\n",
    "\n",
    "                    r = ts2.reward\n",
    "                    d = float(ts2.last())\n",
    "\n",
    "                    obs_buf[i,t]  = state\n",
    "                    act_buf[i,t]  = action\n",
    "                    logp_buf[i,t] = logp\n",
    "                    val_buf[i,t]  = value\n",
    "                    rew_buf[i,t]  = r\n",
    "                    done_buf[i,t] = d\n",
    "\n",
    "                    if d:\n",
    "                        total = rew_buf[i,:t+1].sum().item()\n",
    "                        episode_rewards.append(total)\n",
    "                        envs.reset(i)\n",
    "\n",
    "            # bootstrap value for last state\n",
    "            for i in range(NB_ACTORS):\n",
    "                last_state = preprocess(envs.obs[i])\n",
    "                val_buf[i,T] = model(last_state)[1]\n",
    "\n",
    "        # ─── Compute GAE and advantages ──────────────────────────────────────────\n",
    "        for i in range(NB_ACTORS):\n",
    "            gae = 0\n",
    "            for t in reversed(range(T)):\n",
    "                mask = 1.0 - done_buf[i,t]\n",
    "                delta = rew_buf[i,t] + GAMMA * val_buf[i,t+1] * mask - val_buf[i,t]\n",
    "                gae = delta + GAMMA * GAE_LAMBDA * mask * gae\n",
    "                adv_buf[i,t] = gae\n",
    "\n",
    "        # flatten batches\n",
    "        b_s  = obs_buf.reshape(-1, 2, SCREEN_SIZE, SCREEN_SIZE)\n",
    "        b_a  = act_buf.reshape(-1)\n",
    "        b_lp = logp_buf.reshape(-1)\n",
    "        b_v  = val_buf[:,:T].reshape(-1)\n",
    "        b_ad = adv_buf.reshape(-1)\n",
    "\n",
    "        # ─── PPO update ─────────────────────────────────────────────────────────\n",
    "        for _ in range(K):\n",
    "            dataset = TensorDataset(b_s, b_a, b_lp, b_v, b_ad)\n",
    "            loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            for st, ac, old_lp, old_v, adv in loader:\n",
    "                logits, value = model(st)\n",
    "                dist = Categorical(logits=logits)\n",
    "\n",
    "                lp = dist.log_prob(ac)\n",
    "                ratio = torch.exp(lp - old_lp)\n",
    "\n",
    "                clip = 0.1 * (1 - it / MAX_ITERS)\n",
    "                obj1 = adv * ratio\n",
    "                obj2 = adv * torch.clamp(ratio, 1-clip, 1+clip)\n",
    "                p_loss = -torch.min(obj1, obj2).mean()\n",
    "\n",
    "                ret    = adv + old_v\n",
    "                v1     = (value - ret).pow(2)\n",
    "                v2     = (torch.clamp(value, old_v-clip, old_v+clip) - ret).pow(2)\n",
    "                v_loss = 0.5 * torch.max(v1, v2).mean()\n",
    "\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss = p_loss + VF_COEF * v_loss - ENT_COEF * entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # ─── Save final replays ────────────────────────────────────────────────────\n",
    "    os.makedirs(REPLAY_DIR, exist_ok=True)\n",
    "    for i in range(NB_ACTORS):\n",
    "        path = os.path.join(REPLAY_DIR, f\"ppo_final_{i}.SC2Replay\")\n",
    "        envs.envs[i]._save_replay(\"PPO\", path)\n",
    "    logger.info(\"💾 Saved final replay(s) to %s\", REPLAY_DIR)\n",
    "\n",
    "    # ─── Plot learning curve ───────────────────────────────────────────────────\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(episode_rewards, label=\"Episode reward\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"PPO Learning Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    envs.close()\n",
    "    logger.info(\"✅ PPO training complete over %d episodes\", len(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bdc789-8c2a-4ec5-b5aa-aeac4bc4c14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87a1e3-1c57-41da-a461-9c6a4a0b6d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:38:06 [INFO] Initializing 1 SC2 env(s)…\n",
      "22:38:06 [INFO] Launching SC2: D:\\Games\\StarCraft II\\Versions/Base94137\\SC2_x64.exe -listen 127.0.0.1 -port 54188 -dataDir D:\\Games\\StarCraft II\\ -tempDir C:\\Users\\svarp\\AppData\\Local\\Temp\\sc-pquu030c\\ -displayMode 0 -windowwidth 640 -windowheight 480 -windowx 50 -windowy 50\n",
      "22:38:06 [INFO] Connecting to: ws://127.0.0.1:54188/sc2api, attempt: 0, running: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️  Loading checkpoint…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:38:09 [INFO] Connecting to: ws://127.0.0.1:54188/sc2api, attempt: 1, running: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "import matplotlib.pyplot as plt\n",
    "from absl import flags\n",
    "from pysc2.lib import actions as sc2_actions\n",
    "\n",
    "# from environment import SC2EnvsMulti\n",
    "# from utils      import preprocess, legal_actions, make_pysc2_call, safe_coords\n",
    "# from model      import ActorCritic        # <— your model class\n",
    "# from config     import (\n",
    "#     NUM_EPISODES, NB_ACTORS, SCREEN_SIZE,\n",
    "#     LR, REPLAY_DIR, REPLAY_PREFIX\n",
    "# )\n",
    "\n",
    "NUM_EPISODES  = 200\n",
    "NB_ACTORS     = 1\n",
    "REPLAY_DIR    = \"replays\"\n",
    "REPLAY_PREFIX = \"pysc2_run\"\n",
    "\n",
    "\n",
    "# ─── Fix flags ────────────────────────────────────────────────────────────────\n",
    "flags.FLAGS(sys.argv, known_only=True)\n",
    "\n",
    "# ─── Lookup Function ID → Name ───────────────────────────────────────────────\n",
    "FUNC_ID_TO_NAME = {f.id: f.name for f in sc2_actions.FUNCTIONS}\n",
    "\n",
    "# ─── Checkpoint setup ─────────────────────────────────────────────────────────\n",
    "CHECKPOINT_PATH = \"ppo_checkpoint.pth\"\n",
    "\n",
    "NUM_ACTIONS = len(ACTION_LIST)  # = 8 if you added 'select' + 7 others\n",
    "\n",
    "model = ActorCritic(in_channels=2, nb_actions=NUM_ACTIONS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(\"▶️  Loading checkpoint…\")\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"opt_state\"])\n",
    "    start_ep = ckpt[\"episode\"] + 1\n",
    "else:\n",
    "    start_ep = 1\n",
    "\n",
    "# ─── Environment & UI setup ──────────────────────────────────────────────────\n",
    "console        = Console()\n",
    "envs           = SC2EnvsMulti(\n",
    "    nb_actor=NB_ACTORS,\n",
    "    replay_dir=REPLAY_DIR,\n",
    "    replay_prefix=REPLAY_PREFIX,\n",
    ")\n",
    "pending_action = [None] * NB_ACTORS\n",
    "recent_rows    = deque(maxlen=20)\n",
    "scores         = []\n",
    "\n",
    "def generate_table():\n",
    "    table = Table(title=f\"SC2 Agent Actions (Last {len(recent_rows)} Steps)\", expand=True)\n",
    "    table.add_column(\"Step\",        justify=\"right\")\n",
    "    table.add_column(\"Func ID\",     justify=\"right\")\n",
    "    table.add_column(\"Action Name\", justify=\"left\")\n",
    "    table.add_column(\"Args\",        justify=\"left\")\n",
    "    for row in recent_rows:\n",
    "        table.add_row(*row)\n",
    "    return table\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ─── Main Episode Loop ────────────────────────────────────────────────────────\n",
    "# with Live(generate_table(), refresh_per_second=10, console=console, transient=True) as live:\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── Main Episode Loop with TQDM & Static Table ─────────────────────────────\n",
    "pending_action = [None] * NB_ACTORS\n",
    "recent_rows    = deque(maxlen=20)\n",
    "scores         = []\n",
    "\n",
    "for ep in tqdm(range(start_ep, NUM_EPISODES + 1), desc=\"Episodes\", unit=\"ep\"):\n",
    "    # reset envs & trackers\n",
    "    for i in range(NB_ACTORS):\n",
    "        envs.reset(i)\n",
    "    episode_score = [0] * NB_ACTORS\n",
    "    step = 0\n",
    "    console.log(f\"[blue]=== Episode {ep} ===[/blue]\")\n",
    "\n",
    "    # clear the last-20 buffer\n",
    "    recent_rows.clear()\n",
    "\n",
    "    while True:\n",
    "        for i in range(NB_ACTORS):\n",
    "            ts = envs.obs[i]\n",
    "\n",
    "            # 1) Fire pending or sample new\n",
    "            if pending_action[i]:\n",
    "                fn_call, pending_action[i] = make_pysc2_call(None, ts, pending_action[i])\n",
    "            else:\n",
    "                idx = random.choice(legal_actions(ts))\n",
    "                fn_call, pending_action[i] = make_pysc2_call(idx, ts)\n",
    "\n",
    "            # 2) Step env & collect reward\n",
    "            ts2 = envs.step(i, fn_call)\n",
    "            episode_score[i] += ts2.reward\n",
    "\n",
    "            # 3) Log into your deque\n",
    "            fid      = str(fn_call.function)\n",
    "            fname    = FUNC_ID_TO_NAME.get(fn_call.function, \"UNKNOWN\")\n",
    "            args_str = str(fn_call.arguments)\n",
    "            recent_rows.append((str(step), fid, fname, args_str))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # ─── Save checkpoint at end of episode ────────────────────────────────\n",
    "        torch.save({\n",
    "            \"episode\":     ep,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"opt_state\":   optimizer.state_dict(),\n",
    "        }, CHECKPOINT_PATH)\n",
    "        console.log(f\"[yellow]Checkpoint saved at episode {ep}[/yellow]\")\n",
    "\n",
    "    # 4) Print your last-20 actions table (static)\n",
    "    console.print(generate_table())\n",
    "\n",
    "    # 5) Save checkpoint\n",
    "    torch.save({\n",
    "        \"episode\":     ep,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"opt_state\":   optimizer.state_dict(),\n",
    "    }, CHECKPOINT_PATH)\n",
    "    console.log(f\"[yellow]Checkpoint saved at episode {ep}[/yellow]\")\n",
    "\n",
    "\n",
    "# ─── Cleanup & Plot ─────────────────────────────────────────────────────────\n",
    "envs.close()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(scores, marker=\"o\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Agent Performance per Episode\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ab78a-88dd-4877-a51c-46588736a5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204dc52-7157-4d41-9c48-52bba64e0baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202095a-e051-4db0-8711-0b827819118a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
