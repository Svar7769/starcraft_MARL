{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e76814-2083-496f-9cba-835f02a7fa47",
   "metadata": {},
   "source": [
    "# PPO- based Pysc2\n",
    "\n",
    "Modular Design\n",
    "sc2_ppo_project/\n",
    "\n",
    "â”œâ”€â”€ main.py             # Entry-point to run training\n",
    "\n",
    "â”œâ”€â”€ config.py           # Hyperparameters and logging config\n",
    "\n",
    "â”œâ”€â”€ environment.py      # SC2 environment wrapper\n",
    "\n",
    "â”œâ”€â”€ model.py            # Actor-Critic neural network\n",
    "\n",
    "â”œâ”€â”€ utils.py            # Observation preprocessing and action utilities\n",
    "\n",
    "â””â”€â”€ ppo.py              # PPO training algorithm implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f92e15-9658-4ec9-8e6c-2b6e7769257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9344929d-431d-4821-8768-35f16cb59ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "# â”€â”€â”€ Hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MAP_NAME     = \"CollectMineralsAndGas\"\n",
    "MAP_NAME     = \"Simple64\"\n",
    "\n",
    "SCREEN_SIZE  = 84\n",
    "MINIMAP_SIZE = 64\n",
    "STEP_MUL     = 16\n",
    "NB_ACTORS    = 1\n",
    "T            = 128\n",
    "K            = 10\n",
    "BATCH_SIZE   = 256\n",
    "GAMMA        = 0.99\n",
    "GAE_LAMBDA   = 0.95\n",
    "LR           = 2.5e-4\n",
    "ENT_COEF     = 0.01\n",
    "VF_COEF      = 1.0\n",
    "MAX_ITERS    = 10000\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# â”€â”€â”€ Logging Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c30bd3-1eff-4263-93d8-d42fc320ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "656af9d7-6973-448b-b4f6-a6ca44b13d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features\n",
    "# from config import MAP_NAME, SCREEN_SIZE, MINIMAP_SIZE, STEP_MUL, logger\n",
    "\n",
    "class SC2Envs:\n",
    "    def __init__(self, nb_actor):\n",
    "        logger.info(\"Initializing %d SC2 env(s)...\", nb_actor)\n",
    "        self.nb   = nb_actor\n",
    "        self.envs = [self._make_env() for _ in range(nb_actor)]\n",
    "        self.obs  = [None]*nb_actor\n",
    "        self.done = [False]*nb_actor\n",
    "        self._init_all()\n",
    "        logger.info(\"All SC2 env(s) ready.\")\n",
    "\n",
    "    def _make_env(self):\n",
    "        return sc2_env.SC2Env(\n",
    "            map_name=MAP_NAME,\n",
    "            players=[sc2_env.Agent(sc2_env.Race.terran)],\n",
    "            agent_interface_format=features.AgentInterfaceFormat(\n",
    "                feature_dimensions=features.Dimensions(\n",
    "                    screen=SCREEN_SIZE, minimap=MINIMAP_SIZE),\n",
    "                use_feature_units=True,\n",
    "                use_raw_units=False,\n",
    "                use_camera_position=True,\n",
    "                action_space=actions.ActionSpace.FEATURES\n",
    "            ),\n",
    "            step_mul=STEP_MUL,\n",
    "            game_steps_per_episode=0,\n",
    "            visualize=False,\n",
    "        )\n",
    "\n",
    "    def _init_all(self):\n",
    "        for i, e in enumerate(self.envs):\n",
    "            ts = e.reset()[0]\n",
    "            self.obs[i], self.done[i] = ts, False\n",
    "\n",
    "    def reset(self, i):\n",
    "        ts = self.envs[i].reset()[0]\n",
    "        self.obs[i], self.done[i] = ts, False\n",
    "        return ts\n",
    "\n",
    "    def step(self, i, fc):\n",
    "        ts = self.envs[i].step([fc])[0]\n",
    "        self.obs[i], self.done[i] = ts, ts.last()\n",
    "        return ts\n",
    "\n",
    "    def close(self):\n",
    "        for e in self.envs:\n",
    "            e.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "009ecfc0-4629-46d1-9c03-55c6733f10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Plaer Map\n",
    "\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features\n",
    "import logging\n",
    "\n",
    "class SC2EnvsMulti:\n",
    "    def __init__(self, nb_actor):\n",
    "        logger.info(\"Initializing %d SC2 env(s)...\", nb_actor)\n",
    "        self.nb = nb_actor\n",
    "        self.envs = [self._make_env() for _ in range(nb_actor)]\n",
    "        self.obs = [env.reset()[0] for env in self.envs]\n",
    "        self.done = [False] * nb_actor\n",
    "\n",
    "    def _make_env(self):\n",
    "        return sc2_env.SC2Env(\n",
    "            map_name=MAP_NAME,\n",
    "            players=[\n",
    "                sc2_env.Agent(sc2_env.Race.terran),\n",
    "                sc2_env.Bot(sc2_env.Race.terran, difficulty)\n",
    "\n",
    "            ],\n",
    "            agent_interface_format=features.AgentInterfaceFormat(\n",
    "                feature_dimensions=features.Dimensions(\n",
    "                    screen=SCREEN_SIZE, minimap=MINIMAP_SIZE),\n",
    "                use_feature_units=True,\n",
    "                use_raw_units=False,\n",
    "                use_camera_position=True,\n",
    "                action_space=actions.ActionSpace.FEATURES\n",
    "            ),\n",
    "            step_mul=STEP_MUL,\n",
    "            game_steps_per_episode=0,\n",
    "            visualize=False\n",
    "        )\n",
    "\n",
    "    def step(self, i, action):\n",
    "        timestep = self.envs[i].step([action])[0]\n",
    "        self.obs[i] = timestep\n",
    "        self.done[i] = timestep.last()\n",
    "        return timestep\n",
    "\n",
    "    def reset(self, i):\n",
    "        self.obs[i] = self.envs[i].reset()[0]\n",
    "        self.done[i] = False\n",
    "\n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f776099f-7adf-49ec-b782-9b7d1f38d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d359419f-72a0-4747-aeec-d3cdb502bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from config import SCREEN_SIZE, DEVICE\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, in_channels, nb_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 8, stride=4), nn.Tanh(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2), nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, SCREEN_SIZE, SCREEN_SIZE)\n",
    "            conv_out = self.conv(dummy).shape[-1]\n",
    "\n",
    "        self.fc     = nn.Sequential(nn.Linear(conv_out, 256), nn.Tanh())\n",
    "        self.actor  = nn.Linear(256, nb_actions)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = self.fc(h)\n",
    "        return self.actor(h), self.critic(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c70587f2-7223-4547-afe0-6a9f1c26f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28143d60-a430-4f01-829a-f1278b94f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from pysc2.lib import actions, features\n",
    "# # from config import DEVICE\n",
    "\n",
    "# _PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "# _UNIT_TYPE       = features.SCREEN_FEATURES.unit_type.index\n",
    "\n",
    "# ACTION_LIST = ['do_nothing', 'select_idle', 'build_refinery', 'harvest']\n",
    "# FUNC_ID = {\n",
    "#     'do_nothing': actions.FUNCTIONS.no_op.id,\n",
    "#     'select_idle': actions.FUNCTIONS.select_idle_worker.id,\n",
    "#     'build_refinery': actions.FUNCTIONS.Build_Refinery_screen.id,\n",
    "#     'harvest': actions.FUNCTIONS.Harvest_Gather_screen.id,\n",
    "# }\n",
    "\n",
    "# def preprocess(ts):\n",
    "#     fs = ts.observation.feature_screen\n",
    "#     pr = fs[_PLAYER_RELATIVE].astype(np.float32) / 4.0\n",
    "#     ut = fs[_UNIT_TYPE].astype(np.float32) / fs[_UNIT_TYPE].max()\n",
    "#     stacked = np.stack([pr, ut], axis=0)\n",
    "#     return torch.from_numpy(stacked).unsqueeze(0).float().to(DEVICE)\n",
    "\n",
    "# def legal_actions(ts):\n",
    "#     avail = set(ts.observation.available_actions)\n",
    "#     fus   = ts.observation.feature_units\n",
    "#     legal = [0]\n",
    "#     if FUNC_ID['select_idle'] in avail: legal.append(1)\n",
    "#     if FUNC_ID['build_refinery'] in avail and any(u.unit_type==342 for u in fus): legal.append(2)\n",
    "#     if FUNC_ID['harvest'] in avail and any(u.unit_type==341 for u in fus): legal.append(3)\n",
    "#     return legal\n",
    "\n",
    "# def make_pysc2_call(action_idx, ts):\n",
    "#     name, fid = ACTION_LIST[action_idx], FUNC_ID[ACTION_LIST[action_idx]]\n",
    "#     if name == 'select_idle':\n",
    "#         return actions.FunctionCall(fid, [[2]])\n",
    "#     if name in ('build_refinery','harvest'):\n",
    "#         fus = ts.observation.feature_units\n",
    "#         cand = [u for u in fus if (u.unit_type==342 if name=='build_refinery' else u.unit_type==341)]\n",
    "#         if not cand:\n",
    "#             return actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "#         u = random.choice(cand)\n",
    "#         return actions.FunctionCall(fid, [[0],[u.x,u.y]])\n",
    "#     return actions.FunctionCall(fid, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9259464-83a6-4c0d-a9a9-7b536dd1af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from pysc2.lib import actions, features\n",
    "\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index\n",
    "\n",
    "ACTION_LIST = ['do_nothing', 'move', 'attack', 'build', 'gather', 'upgrade', 'train']\n",
    "ACTION_INDEX = {name: idx for idx, name in enumerate(ACTION_LIST)}\n",
    "\n",
    "SCREEN_SIZE = 84\n",
    "\n",
    "# Terran building unit_type IDs (partial list â€” expand as needed)\n",
    "TERRAN_STRUCTURE_TYPES = [\n",
    "    18,   # CommandCenter\n",
    "    20,   # SupplyDepot\n",
    "    21,   # Barracks\n",
    "    22,   # EngineeringBay\n",
    "    23,   # MissileTurret\n",
    "    24,   # Bunker\n",
    "    25,   # Refinery\n",
    "    27,   # Factory\n",
    "    28,   # GhostAcademy\n",
    "    29,   # Starport\n",
    "    30,   # Armory\n",
    "    130, 131, 132, 133  # Tech lab, Reactor, etc.\n",
    "]\n",
    "\n",
    "def safe_coords(x, y, screen_size=SCREEN_SIZE):\n",
    "    x = max(0, min(screen_size - 1, x))\n",
    "    y = max(0, min(screen_size - 1, y))\n",
    "    return [x, y]\n",
    "\n",
    "def preprocess(ts):\n",
    "    fs = ts.observation.feature_screen\n",
    "    pr = fs[_PLAYER_RELATIVE].astype(np.float32) / 4.0\n",
    "    ut = fs[_UNIT_TYPE].astype(np.float32) / fs[_UNIT_TYPE].max()\n",
    "    stacked = np.stack([pr, ut], axis=0)\n",
    "    return torch.from_numpy(stacked).unsqueeze(0).float()\n",
    "\n",
    "def legal_actions(ts):\n",
    "    avail = set(ts.observation.available_actions)\n",
    "    fus = ts.observation.feature_units\n",
    "    legal = [ACTION_INDEX['do_nothing']]\n",
    "\n",
    "    if actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        legal.append(ACTION_INDEX['move'])\n",
    "\n",
    "    if actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        legal.append(ACTION_INDEX['attack'])\n",
    "\n",
    "    if any('Build' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['build'])\n",
    "\n",
    "    if actions.FUNCTIONS.Harvest_Gather_screen.id in avail and any(u.unit_type == 341 for u in fus):\n",
    "        legal.append(ACTION_INDEX['gather'])\n",
    "\n",
    "    if any('Research' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['upgrade'])\n",
    "\n",
    "    if any('Train' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['train'])\n",
    "\n",
    "    return legal\n",
    "\n",
    "def make_pysc2_call(action_idx, ts, pending=None):\n",
    "    obs = ts.observation\n",
    "    fus = obs.feature_units\n",
    "    avail = set(obs.available_actions)\n",
    "\n",
    "    if pending:\n",
    "        if pending['action_fn'] in avail:\n",
    "            args = pending['args']\n",
    "            if len(args) > 1 and isinstance(args[1], list) and len(args[1]) == 2:\n",
    "                x, y = args[1]\n",
    "                return actions.FunctionCall(pending['action_fn'], [args[0], safe_coords(x, y)]), None\n",
    "            else:\n",
    "                return actions.FunctionCall(pending['action_fn'], args), None\n",
    "        else:\n",
    "            print(f\"[SKIP] Function {pending['action_fn']} not available anymore.\")\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    # â”€â”€ Train: Select building first â”€â”€\n",
    "    if action_idx == ACTION_INDEX['train']:\n",
    "        building_units = [u for u in fus if u.alliance == features.PlayerRelative.SELF and u.unit_type in TERRAN_STRUCTURE_TYPES]\n",
    "        if not building_units or actions.FUNCTIONS.select_point.id not in avail:\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "        building = random.choice(building_units)\n",
    "        select_coords = safe_coords(building.x, building.y)\n",
    "        select_action = actions.FunctionCall(actions.FUNCTIONS.select_point.id, [[0], select_coords])\n",
    "\n",
    "        train_actions = [a for a in avail if 'Train' in actions.FUNCTIONS[a].name]\n",
    "        if train_actions:\n",
    "            train_action = random.choice(train_actions)\n",
    "            next_action = {'action_fn': train_action, 'args': [[0]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "        return select_action, next_action\n",
    "\n",
    "    # â”€â”€ Default: Select unit â”€â”€\n",
    "    selectable_units = [u for u in fus if u.alliance == features.PlayerRelative.SELF]\n",
    "    if not selectable_units or actions.FUNCTIONS.select_point.id not in avail:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    unit = random.choice(selectable_units)\n",
    "    select_coords = safe_coords(unit.x, unit.y)\n",
    "    select_action = actions.FunctionCall(actions.FUNCTIONS.select_point.id, [[0], select_coords])\n",
    "\n",
    "    if action_idx == ACTION_INDEX['move'] and actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        x, y = np.random.randint(0, SCREEN_SIZE), np.random.randint(0, SCREEN_SIZE)\n",
    "        next_action = {'action_fn': actions.FUNCTIONS.Move_screen.id, 'args': [[0], [x, y]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['attack'] and actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        enemies = [u for u in fus if u.alliance == features.PlayerRelative.ENEMY]\n",
    "        if enemies:\n",
    "            target = random.choice(enemies)\n",
    "            next_action = {'action_fn': actions.FUNCTIONS.Attack_screen.id, 'args': [[0], [target.x, target.y]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['gather'] and actions.FUNCTIONS.Harvest_Gather_screen.id in avail:\n",
    "        minerals = [u for u in fus if u.unit_type == 341]\n",
    "        if minerals:\n",
    "            target = random.choice(minerals)\n",
    "            next_action = {'action_fn': actions.FUNCTIONS.Harvest_Gather_screen.id, 'args': [[0], [target.x, target.y]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['build']:\n",
    "        build_actions = [a for a in avail if 'Build' in actions.FUNCTIONS[a].name]\n",
    "        if build_actions:\n",
    "            build_action = random.choice(build_actions)\n",
    "            buildable = np.argwhere(obs.feature_screen.buildable == 1)\n",
    "            if buildable.size > 0:\n",
    "                y, x = random.choice(buildable)\n",
    "                next_action = {'action_fn': build_action, 'args': [[0], [x, y]]}\n",
    "            else:\n",
    "                next_action = None\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['upgrade']:\n",
    "        upgrade_actions = [a for a in avail if 'Research' in actions.FUNCTIONS[a].name]\n",
    "        if upgrade_actions:\n",
    "            upgrade_action = random.choice(upgrade_actions)\n",
    "            next_action = {'action_fn': upgrade_action, 'args': [[0]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    else:\n",
    "        next_action = None\n",
    "\n",
    "    return select_action, next_action\n",
    "\n",
    "def make_pysc2_call_core(action_idx, ts):\n",
    "    obs = ts.observation\n",
    "    fus = obs.feature_units\n",
    "    avail = set(obs.available_actions)\n",
    "\n",
    "    if action_idx == ACTION_INDEX['do_nothing']:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['move'] and actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        x, y = np.random.randint(0, SCREEN_SIZE), np.random.randint(0, SCREEN_SIZE)\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.Move_screen.id, [[0], safe_coords(x, y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['attack'] and actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        enemies = [u for u in fus if u.alliance == features.PlayerRelative.ENEMY]\n",
    "        if enemies:\n",
    "            target = random.choice(enemies)\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.Attack_screen.id, [[0], safe_coords(target.x, target.y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['build']:\n",
    "        build_actions = [a for a in avail if 'Build' in actions.FUNCTIONS[a].name]\n",
    "        if build_actions:\n",
    "            build_action = random.choice(build_actions)\n",
    "            buildable = np.argwhere(obs.feature_screen.buildable == 1)\n",
    "            if buildable.size > 0:\n",
    "                y, x = random.choice(buildable)\n",
    "                return actions.FunctionCall(build_action, [[0], safe_coords(x, y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['gather'] and actions.FUNCTIONS.Harvest_Gather_screen.id in avail:\n",
    "        minerals = [u for u in fus if u.unit_type == 341]\n",
    "        if minerals:\n",
    "            target = random.choice(minerals)\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.Harvest_Gather_screen.id, [[0], safe_coords(target.x, target.y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['upgrade']:\n",
    "        upgrade_actions = [a for a in avail if 'Research' in actions.FUNCTIONS[a].name]\n",
    "        if upgrade_actions:\n",
    "            upgrade_action = random.choice(upgrade_actions)\n",
    "            return actions.FunctionCall(upgrade_action, [[0]]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['train']:\n",
    "        train_actions = [a for a in avail if 'Train' in actions.FUNCTIONS[a].name]\n",
    "        if train_actions:\n",
    "            train_action = random.choice(train_actions)\n",
    "            return actions.FunctionCall(train_action, [[0]]), None\n",
    "\n",
    "    return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95d4b4d6-c7fb-47f8-9b29-89ca20299c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO training LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f710a40-241f-4cef-91a8-4413a650d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pysc2.lib import actions\n",
    "from config import *\n",
    "# from utils import preprocess, legal_actions, make_pysc2_call\n",
    "\n",
    "\n",
    "# â”€â”€â”€ PPO Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def PPO(envs, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1.0, end_factor=0.0, total_iters=MAX_ITERS\n",
    "    )\n",
    "\n",
    "    ep_rewards = []       # final cumulative score of each episode\n",
    "    last_score = [0]*envs.nb\n",
    "\n",
    "    logger.info(\"â–¶ï¸  Starting PPO for %d iterations\", MAX_ITERS)\n",
    "    for it in range(MAX_ITERS):\n",
    "        if it % 1000 == 0:\n",
    "            logger.info(\"ðŸ”„ Iter %d / %d\", it, MAX_ITERS)\n",
    "\n",
    "        # storage buffers\n",
    "        obs_buf  = torch.zeros(envs.nb, T, 2, SCREEN_SIZE, SCREEN_SIZE, device=DEVICE)\n",
    "        act_buf  = torch.zeros(envs.nb, T,      dtype=torch.long, device=DEVICE)\n",
    "        logp_buf = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        val_buf  = torch.zeros(envs.nb, T+1,                   device=DEVICE)\n",
    "        rew_buf  = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        done_buf = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        adv_buf  = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "\n",
    "        # â”€â”€â”€ Rollout â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        with torch.no_grad():\n",
    "            for t in range(T):\n",
    "                for i in range(envs.nb):\n",
    "                    ts    = envs.obs[i]\n",
    "                    state = preprocess(ts)\n",
    "                    logits, value = model(state)\n",
    "\n",
    "                    # mask illegal\n",
    "                    LA   = legal_actions(ts)\n",
    "                    mask = torch.full_like(logits, float('-inf'))\n",
    "                    mask[0, LA] = 0.0\n",
    "                    dist = Categorical(logits=logits + mask)\n",
    "\n",
    "                    action = dist.sample()\n",
    "                    logp   = dist.log_prob(action)\n",
    "                    fc     = make_pysc2_call(action.item(), ts)\n",
    "\n",
    "                    # step (fallback to no-op)\n",
    "                    try:\n",
    "                        ts2 = envs.step(i, fc)\n",
    "                    except ValueError:\n",
    "                        ts2 = envs.step(i,\n",
    "                            actions.FunctionCall(actions.FUNCTIONS.no_op.id, []))\n",
    "\n",
    "                    # â”€â”€ reward = Î” score_cumulative â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                    cur_score = int(ts2.observation['score_cumulative'][0])\n",
    "                    r = cur_score - last_score[i]\n",
    "                    last_score[i] = cur_score\n",
    "\n",
    "                    d = float(ts2.last())\n",
    "\n",
    "                    obs_buf[i,t]  = state\n",
    "                    act_buf[i,t]  = action\n",
    "                    logp_buf[i,t] = logp\n",
    "                    val_buf[i,t]  = value\n",
    "                    rew_buf[i,t]  = r\n",
    "                    done_buf[i,t] = d\n",
    "\n",
    "                    if d:\n",
    "                        ep_rewards.append(cur_score)\n",
    "                        last_score[i] = 0\n",
    "                        envs.reset(i)\n",
    "\n",
    "            # bootstrap final value\n",
    "            for i in range(envs.nb):\n",
    "                val_buf[i,T] = model(preprocess(envs.obs[i]))[1]\n",
    "\n",
    "        # â”€â”€â”€ GAE & flatten â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        for i in range(envs.nb):\n",
    "            gae = 0\n",
    "            for t in reversed(range(T)):\n",
    "                mask  = 1.0 - done_buf[i,t]\n",
    "                delta = rew_buf[i,t] + GAMMA*val_buf[i,t+1]*mask - val_buf[i,t]\n",
    "                gae   = delta + GAMMA*GAE_LAMBDA*mask*gae\n",
    "                adv_buf[i,t] = gae\n",
    "\n",
    "        b_s  = obs_buf.reshape(-1,2,SCREEN_SIZE,SCREEN_SIZE)\n",
    "        b_a  = act_buf.reshape(-1)\n",
    "        b_lp = logp_buf.reshape(-1)\n",
    "        b_v  = val_buf[:,:T].reshape(-1)\n",
    "        b_ad = adv_buf.reshape(-1)\n",
    "\n",
    "        # â”€â”€â”€ PPO updates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        for _ in range(K):\n",
    "            ds     = TensorDataset(b_s,b_a,b_lp,b_v,b_ad)\n",
    "            loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            for st, ac, old_lp, old_v, adv in loader:\n",
    "                logits, val = model(st)\n",
    "                dist        = Categorical(logits=logits)\n",
    "                lp          = dist.log_prob(ac)\n",
    "                ratio       = torch.exp(lp - old_lp)\n",
    "\n",
    "                clip   = 0.1 * (1 - it/MAX_ITERS)\n",
    "                obj1   = adv * ratio\n",
    "                obj2   = adv * torch.clamp(ratio, 1-clip, 1+clip)\n",
    "                p_loss = -torch.min(obj1,obj2).mean()\n",
    "\n",
    "                ret     = adv + old_v\n",
    "                v1      = (val - ret).pow(2)\n",
    "                v2      = (torch.clamp(val,old_v-clip,old_v+clip)-ret).pow(2)\n",
    "                v_loss  = 0.5 * torch.max(v1,v2).mean()\n",
    "\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss    = p_loss + VF_COEF*v_loss - ENT_COEF*entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # â”€â”€â”€ Plot learning curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(ep_rewards, label=\"final score per episode\")\n",
    "    plt.title(\"CollectMineralsAndGas: cumulative score\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    envs.close()\n",
    "    logger.info(\"âœ… Training complete\")\n",
    "    logger.info(f\"Saved learning_curve.png over {len(ep_rewards)} episodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdcecead-f59c-4bbd-827d-771ad655c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47d18d4c-cd99-477a-be83-74fd6005978b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from absl import app\n",
    "# # from environment import SC2Envs\n",
    "# # from model import ActorCritic\n",
    "# # from ppo import PPO\n",
    "# # from config import NB_ACTORS, DEVICE\n",
    "# # from utils import ACTION_LIST\n",
    "\n",
    "# def main(_):\n",
    "#     envs = SC2Envs(NB_ACTORS)\n",
    "#     model = ActorCritic(2, len(ACTION_LIST)).to(DEVICE)\n",
    "#     PPO(envs, model)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#     sys.argv = sys.argv[:1]  # Remove extra flags passed by Jupyter or IPython\n",
    "#     app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "942d6802-c091-4439-9f6c-a28b16ccbb30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:26:31 [INFO] Initializing 1 SC2 env(s)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Maps support 2 - 2 players, but trying to join with 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# from util import preprocess, legal_actions, make_pysc2_call\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# from env import SC2Envs\u001b[39;00m\n\u001b[32m     14\u001b[39m console = Console()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m envs = \u001b[43mSC2Envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_actor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m pending_action = [\u001b[38;5;28;01mNone\u001b[39;00m] * envs.nb\n\u001b[32m     18\u001b[39m MAX_ROWS = \u001b[32m20\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSC2Envs.__init__\u001b[39m\u001b[34m(self, nb_actor)\u001b[39m\n\u001b[32m      7\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m SC2 env(s)...\u001b[39m\u001b[33m\"\u001b[39m, nb_actor)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.nb   = nb_actor\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mself\u001b[39m.envs = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnb_actor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.obs  = [\u001b[38;5;28;01mNone\u001b[39;00m]*nb_actor\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.done = [\u001b[38;5;28;01mFalse\u001b[39;00m]*nb_actor\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m SC2 env(s)...\u001b[39m\u001b[33m\"\u001b[39m, nb_actor)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.nb   = nb_actor\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mself\u001b[39m.envs = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_actor)]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.obs  = [\u001b[38;5;28;01mNone\u001b[39;00m]*nb_actor\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.done = [\u001b[38;5;28;01mFalse\u001b[39;00m]*nb_actor\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mSC2Envs._make_env\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_env\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc2_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSC2Env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmap_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAP_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplayers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43msc2_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc2_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterran\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent_interface_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAgentInterfaceFormat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfeature_dimensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDimensions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                \u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSCREEN_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMINIMAP_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_feature_units\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_raw_units\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_camera_position\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mActionSpace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFEATURES\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep_mul\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTEP_MUL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgame_steps_per_episode\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Project\\starcraft_ai\\venv\\Lib\\site-packages\\pysc2\\env\\sc2_env.py:213\u001b[39m, in \u001b[36mSC2Env.__init__\u001b[39m\u001b[34m(self, map_name, battle_net_map, players, agent_interface_format, discount, discount_zero_after_timeout, visualize, step_mul, realtime, save_replay_episodes, replay_dir, replay_prefix, game_steps_per_episode, score_index, score_multiplier, random_seed, disable_fog, ensure_available_actions, version)\u001b[39m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSingle player maps require exactly one Agent.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m2\u001b[39m <= num_players <= min_players:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    214\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mMaps support 2 - \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m players, but trying to join with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (\n\u001b[32m    215\u001b[39m           min_players, num_players))\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m save_replay_episodes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m replay_dir:\n\u001b[32m    218\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMissing replay_dir\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Maps support 2 - 2 players, but trying to join with 1"
     ]
    }
   ],
   "source": [
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "from absl import flags\n",
    "\n",
    "flags.FLAGS(sys.argv)  # fix required by pysc2\n",
    "# from util import preprocess, legal_actions, make_pysc2_call\n",
    "# from env import SC2Envs\n",
    "\n",
    "console = Console()\n",
    "envs = SC2Envs(nb_actor=1)\n",
    "pending_action = [None] * envs.nb\n",
    "\n",
    "MAX_ROWS = 20\n",
    "recent_rows = deque(maxlen=MAX_ROWS)\n",
    "\n",
    "# For tracking per-episode scores\n",
    "episode_score = [0] * envs.nb\n",
    "scores = []\n",
    "\n",
    "def generate_table():\n",
    "    table = Table(title=f\"SC2 Agent Actions (Last {MAX_ROWS} Steps)\", expand=True)\n",
    "    table.add_column(\"Step\", justify=\"right\")\n",
    "    table.add_column(\"Function ID\", justify=\"right\")\n",
    "    table.add_column(\"Args\", justify=\"left\")\n",
    "    for row in recent_rows:\n",
    "        table.add_row(*row)\n",
    "    return table\n",
    "\n",
    "with Live(generate_table(), refresh_per_second=10, console=console, transient=True) as live:\n",
    "    for step in range(MAX_ITERS):\n",
    "        for i in range(envs.nb):\n",
    "            ts = envs.obs[i]\n",
    "\n",
    "            if pending_action[i]:\n",
    "                action, pending_action[i] = make_pysc2_call(None, ts, pending_action[i])\n",
    "            else:\n",
    "                legal = legal_actions(ts)\n",
    "                action_idx = random.choice(legal)\n",
    "                action, pending_action[i] = make_pysc2_call(action_idx, ts)\n",
    "\n",
    "            recent_rows.append((str(step), str(action.function), str(action.arguments)))\n",
    "            live.update(generate_table())\n",
    "\n",
    "            ts = envs.step(i, action)\n",
    "            episode_score[i] += ts.reward\n",
    "\n",
    "            if ts.last():\n",
    "                scores.append(episode_score[i])\n",
    "                episode_score[i] = 0  # reset\n",
    "                envs.reset(i)\n",
    "\n",
    "envs.close()\n",
    "\n",
    "# Plot episode scores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(scores, label=\"Episode Score\", marker='o', linewidth=1.5)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Score\")\n",
    "plt.title(\"Agent Score per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a1044ec-ea4c-4ed4-91c4-6cede1bb862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Campain Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e3b570e-8f16-4ebf-ba44-aef48488f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:30:10 [INFO] Initializing 1 SC2 env(s)...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pysc2.env.sc2_env' has no attribute 'Computer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# from util import preprocess, legal_actions, make_pysc2_call\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# from env import SC2Envs\u001b[39;00m\n\u001b[32m     14\u001b[39m console = Console()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m envs = \u001b[43mSC2EnvsMulti\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_actor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m pending_action = [\u001b[38;5;28;01mNone\u001b[39;00m] * envs.nb\n\u001b[32m     18\u001b[39m MAX_ROWS = \u001b[32m20\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSC2EnvsMulti.__init__\u001b[39m\u001b[34m(self, nb_actor)\u001b[39m\n\u001b[32m      9\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m SC2 env(s)...\u001b[39m\u001b[33m\"\u001b[39m, nb_actor)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.nb = nb_actor\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mself\u001b[39m.envs = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnb_actor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.obs = [env.reset()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envs]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.done = [\u001b[38;5;28;01mFalse\u001b[39;00m] * nb_actor\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      9\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m SC2 env(s)...\u001b[39m\u001b[33m\"\u001b[39m, nb_actor)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.nb = nb_actor\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mself\u001b[39m.envs = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_actor)]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.obs = [env.reset()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envs]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.done = [\u001b[38;5;28;01mFalse\u001b[39;00m] * nb_actor\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mSC2EnvsMulti._make_env\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_env\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sc2_env.SC2Env(\n\u001b[32m     17\u001b[39m         map_name=MAP_NAME,\n\u001b[32m     18\u001b[39m         players=[\n\u001b[32m     19\u001b[39m             sc2_env.Agent(sc2_env.Race.terran),\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m             \u001b[43msc2_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mComputer\u001b[49m(sc2_env.Race.zerg, sc2_env.Difficulty.very_easy)\n\u001b[32m     21\u001b[39m         ],\n\u001b[32m     22\u001b[39m         agent_interface_format=features.AgentInterfaceFormat(\n\u001b[32m     23\u001b[39m             feature_dimensions=features.Dimensions(\n\u001b[32m     24\u001b[39m                 screen=SCREEN_SIZE, minimap=MINIMAP_SIZE),\n\u001b[32m     25\u001b[39m             use_feature_units=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     26\u001b[39m             use_raw_units=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     27\u001b[39m             use_camera_position=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     28\u001b[39m             action_space=actions.ActionSpace.FEATURES\n\u001b[32m     29\u001b[39m         ),\n\u001b[32m     30\u001b[39m         step_mul=STEP_MUL,\n\u001b[32m     31\u001b[39m         game_steps_per_episode=\u001b[32m0\u001b[39m,\n\u001b[32m     32\u001b[39m         visualize=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     33\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pysc2.env.sc2_env' has no attribute 'Computer'"
     ]
    }
   ],
   "source": [
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "from absl import flags\n",
    "\n",
    "flags.FLAGS(sys.argv)  # fix required by pysc2\n",
    "# from util import preprocess, legal_actions, make_pysc2_call\n",
    "# from env import SC2Envs\n",
    "\n",
    "console = Console()\n",
    "envs = SC2EnvsMulti(nb_actor=1)\n",
    "pending_action = [None] * envs.nb\n",
    "\n",
    "MAX_ROWS = 20\n",
    "recent_rows = deque(maxlen=MAX_ROWS)\n",
    "\n",
    "# For tracking per-episode scores\n",
    "episode_score = [0] * envs.nb\n",
    "scores = []\n",
    "\n",
    "def generate_table():\n",
    "    table = Table(title=f\"SC2 Agent Actions (Last {MAX_ROWS} Steps)\", expand=True)\n",
    "    table.add_column(\"Step\", justify=\"right\")\n",
    "    table.add_column(\"Function ID\", justify=\"right\")\n",
    "    table.add_column(\"Args\", justify=\"left\")\n",
    "    for row in recent_rows:\n",
    "        table.add_row(*row)\n",
    "    return table\n",
    "\n",
    "with Live(generate_table(), refresh_per_second=10, console=console, transient=True) as live:\n",
    "    for step in range(MAX_ITERS):\n",
    "        for i in range(envs.nb):\n",
    "            ts = envs.obs[i]\n",
    "\n",
    "            if pending_action[i]:\n",
    "                action, pending_action[i] = make_pysc2_call(None, ts, pending_action[i])\n",
    "            else:\n",
    "                legal = legal_actions(ts)\n",
    "                action_idx = random.choice(legal)\n",
    "                action, pending_action[i] = make_pysc2_call(action_idx, ts)\n",
    "\n",
    "            recent_rows.append((str(step), str(action.function), str(action.arguments)))\n",
    "            live.update(generate_table())\n",
    "\n",
    "            ts = envs.step(i, action)\n",
    "            episode_score[i] += ts.reward\n",
    "\n",
    "            if ts.last():\n",
    "                scores.append(episode_score[i])\n",
    "                episode_score[i] = 0  # reset\n",
    "                envs.reset(i)\n",
    "\n",
    "envs.close()\n",
    "\n",
    "# Plot episode scores\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(scores, label=\"Episode Score\", marker='o', linewidth=1.5)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Score\")\n",
    "plt.title(\"Agent Score per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87a1e3-1c57-41da-a461-9c6a4a0b6d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
