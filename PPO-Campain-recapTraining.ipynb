{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e76814-2083-496f-9cba-835f02a7fa47",
   "metadata": {},
   "source": [
    "# PPO- based Pysc2\n",
    "\n",
    "Modular Design\n",
    "sc2_ppo_project/\n",
    "\n",
    "├── main.py             # Entry-point to run training\n",
    "\n",
    "├── config.py           # Hyperparameters and logging config\n",
    "\n",
    "├── environment.py      # SC2 environment wrapper\n",
    "\n",
    "├── model.py            # Actor-Critic neural network\n",
    "\n",
    "├── utils.py            # Observation preprocessing and action utilities\n",
    "\n",
    "└── ppo.py              # PPO training algorithm implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f92e15-9658-4ec9-8e6c-2b6e7769257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9344929d-431d-4821-8768-35f16cb59ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ─── Hyperparameters ─────────────────────────────\n",
    "MAP_NAME     = \"Simple64\"\n",
    "\n",
    "SCREEN_SIZE  = 84\n",
    "MINIMAP_SIZE = 64\n",
    "STEP_MUL     = 16\n",
    "NB_ACTORS    = 1\n",
    "T            = 128\n",
    "K            = 10\n",
    "BATCH_SIZE   = 256\n",
    "GAMMA        = 0.99\n",
    "GAE_LAMBDA   = 0.95\n",
    "LR           = 2.5e-4\n",
    "ENT_COEF     = 0.01\n",
    "VF_COEF      = 1.0\n",
    "MAX_ITERS    = 1000\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Replay and Dataset Directories ─────────────\n",
    "REPLAY_DIR   = os.path.join(\"replays\")             # Where to save .SC2Replay files\n",
    "DATASET_PATH = os.path.join(\"dataset.pkl\")         # Where to save (obs, action) dataset\n",
    "\n",
    "# ─── Logging Configuration ──────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure replay directory exists\n",
    "os.makedirs(REPLAY_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c30bd3-1eff-4263-93d8-d42fc320ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656af9d7-6973-448b-b4f6-a6ca44b13d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pysc2.env import sc2_env\n",
    "# from pysc2.lib import actions, features\n",
    "# # from config import MAP_NAME, SCREEN_SIZE, MINIMAP_SIZE, STEP_MUL, logger\n",
    "\n",
    "# class SC2Envs:\n",
    "#     def __init__(self, nb_actor):\n",
    "#         logger.info(\"Initializing %d SC2 env(s)...\", nb_actor)\n",
    "#         self.nb   = nb_actor\n",
    "#         self.envs = [self._make_env() for _ in range(nb_actor)]\n",
    "#         self.obs  = [None]*nb_actor\n",
    "#         self.done = [False]*nb_actor\n",
    "#         self._init_all()\n",
    "#         logger.info(\"All SC2 env(s) ready.\")\n",
    "\n",
    "#     def _make_env(self):\n",
    "#         return sc2_env.SC2Env(\n",
    "#             map_name=MAP_NAME,\n",
    "#             players=[sc2_env.Agent(sc2_env.Race.terran)],\n",
    "#             agent_interface_format=features.AgentInterfaceFormat(\n",
    "#                 feature_dimensions=features.Dimensions(\n",
    "#                     screen=SCREEN_SIZE, minimap=MINIMAP_SIZE),\n",
    "#                 use_feature_units=True,\n",
    "#                 use_raw_units=False,\n",
    "#                 use_camera_position=True,\n",
    "#                 action_space=actions.ActionSpace.FEATURES\n",
    "#             ),\n",
    "#             step_mul=STEP_MUL,\n",
    "#             game_steps_per_episode=0,\n",
    "#             visualize=False,\n",
    "#         )\n",
    "\n",
    "#     def _init_all(self):\n",
    "#         for i, e in enumerate(self.envs):\n",
    "#             ts = e.reset()[0]\n",
    "#             self.obs[i], self.done[i] = ts, False\n",
    "\n",
    "#     def reset(self, i):\n",
    "#         ts = self.envs[i].reset()[0]\n",
    "#         self.obs[i], self.done[i] = ts, False\n",
    "#         return ts\n",
    "\n",
    "#     def step(self, i, fc):\n",
    "#         ts = self.envs[i].step([fc])[0]\n",
    "#         self.obs[i], self.done[i] = ts, ts.last()\n",
    "#         return ts\n",
    "\n",
    "#     def close(self):\n",
    "#         for e in self.envs:\n",
    "#             e.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009ecfc0-4629-46d1-9c03-55c6733f10f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SC2EnvsMulti:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_actor,\n",
    "        replay_dir=\"replays\",\n",
    "        replay_prefix=\"run\",\n",
    "        map_name=\"Simple64\",\n",
    "        screen_size=84,\n",
    "        minimap_size=64,\n",
    "        step_mul=16,\n",
    "    ):\n",
    "        self.nb = nb_actor\n",
    "\n",
    "        # — make replay_dir an absolute folder INSIDE YOUR PROJECT\n",
    "        self.replay_dir = os.path.join(os.getcwd(), replay_dir)\n",
    "        os.makedirs(self.replay_dir, exist_ok=True)\n",
    "\n",
    "        self.replay_prefix = replay_prefix\n",
    "\n",
    "        logger.info(\"Initializing %d SC2 env(s)…\", self.nb)\n",
    "        self.envs = [\n",
    "            self._make_env(map_name, screen_size, minimap_size, step_mul)\n",
    "            for _ in range(self.nb)\n",
    "        ]\n",
    "        self.obs  = [env.reset()[0] for env in self.envs]\n",
    "        self.done = [False] * self.nb\n",
    "\n",
    "    def _make_env(self, map_name, screen_size, minimap_size, step_mul):\n",
    "        return sc2_env.SC2Env(\n",
    "            map_name=map_name,\n",
    "            players=[\n",
    "                sc2_env.Agent(sc2_env.Race.terran),\n",
    "                sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty.very_easy),\n",
    "            ],\n",
    "            agent_interface_format=features.AgentInterfaceFormat(\n",
    "                feature_dimensions=features.Dimensions(\n",
    "                    screen=screen_size, minimap=minimap_size\n",
    "                ),\n",
    "                use_feature_units=True,\n",
    "            ),\n",
    "            step_mul=step_mul,\n",
    "            visualize=False,\n",
    "\n",
    "            # ← now uses the absolute project path\n",
    "            save_replay_episodes=1,\n",
    "            replay_dir=self.replay_dir,\n",
    "            replay_prefix=self.replay_prefix,\n",
    "        )\n",
    "\n",
    "    def step(self, i, action):\n",
    "        ts = self.envs[i].step([action])[0]\n",
    "        self.obs[i]  = ts\n",
    "        self.done[i] = ts.last()\n",
    "        return ts\n",
    "\n",
    "    def reset(self, i):\n",
    "        self.obs[i]  = self.envs[i].reset()[0]\n",
    "        self.done[i] = False\n",
    "\n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f776099f-7adf-49ec-b782-9b7d1f38d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d359419f-72a0-4747-aeec-d3cdb502bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from config import SCREEN_SIZE, DEVICE  # ✅ Import shared config\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, in_channels, nb_actions):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 8, stride=4), nn.Tanh(),\n",
    "            nn.Conv2d(16, 32, 4, stride=2), nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, SCREEN_SIZE, SCREEN_SIZE).to(DEVICE)\n",
    "            conv_out = self.conv(dummy).shape[-1]\n",
    "\n",
    "        self.fc     = nn.Sequential(nn.Linear(conv_out, 256), nn.Tanh())\n",
    "        self.actor  = nn.Linear(256, nb_actions)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = self.fc(h)\n",
    "        return self.actor(h), self.critic(h).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70587f2-7223-4547-afe0-6a9f1c26f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28143d60-a430-4f01-829a-f1278b94f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import random\n",
    "# from pysc2.lib import actions, features\n",
    "# # from config import DEVICE\n",
    "\n",
    "# _PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "# _UNIT_TYPE       = features.SCREEN_FEATURES.unit_type.index\n",
    "\n",
    "# ACTION_LIST = ['do_nothing', 'select_idle', 'build_refinery', 'harvest']\n",
    "# FUNC_ID = {\n",
    "#     'do_nothing': actions.FUNCTIONS.no_op.id,\n",
    "#     'select_idle': actions.FUNCTIONS.select_idle_worker.id,\n",
    "#     'build_refinery': actions.FUNCTIONS.Build_Refinery_screen.id,\n",
    "#     'harvest': actions.FUNCTIONS.Harvest_Gather_screen.id,\n",
    "# }\n",
    "\n",
    "# def preprocess(ts):\n",
    "#     fs = ts.observation.feature_screen\n",
    "#     pr = fs[_PLAYER_RELATIVE].astype(np.float32) / 4.0\n",
    "#     ut = fs[_UNIT_TYPE].astype(np.float32) / fs[_UNIT_TYPE].max()\n",
    "#     stacked = np.stack([pr, ut], axis=0)\n",
    "#     return torch.from_numpy(stacked).unsqueeze(0).float().to(DEVICE)\n",
    "\n",
    "# def legal_actions(ts):\n",
    "#     avail = set(ts.observation.available_actions)\n",
    "#     fus   = ts.observation.feature_units\n",
    "#     legal = [0]\n",
    "#     if FUNC_ID['select_idle'] in avail: legal.append(1)\n",
    "#     if FUNC_ID['build_refinery'] in avail and any(u.unit_type==342 for u in fus): legal.append(2)\n",
    "#     if FUNC_ID['harvest'] in avail and any(u.unit_type==341 for u in fus): legal.append(3)\n",
    "#     return legal\n",
    "\n",
    "# def make_pysc2_call(action_idx, ts):\n",
    "#     name, fid = ACTION_LIST[action_idx], FUNC_ID[ACTION_LIST[action_idx]]\n",
    "#     if name == 'select_idle':\n",
    "#         return actions.FunctionCall(fid, [[2]])\n",
    "#     if name in ('build_refinery','harvest'):\n",
    "#         fus = ts.observation.feature_units\n",
    "#         cand = [u for u in fus if (u.unit_type==342 if name=='build_refinery' else u.unit_type==341)]\n",
    "#         if not cand:\n",
    "#             return actions.FunctionCall(actions.FUNCTIONS.no_op.id, [])\n",
    "#         u = random.choice(cand)\n",
    "#         return actions.FunctionCall(fid, [[0],[u.x,u.y]])\n",
    "#     return actions.FunctionCall(fid, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9259464-83a6-4c0d-a9a9-7b536dd1af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from pysc2.lib import actions, features\n",
    "\n",
    "_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index\n",
    "\n",
    "ACTION_LIST = ['do_nothing', 'move', 'attack', 'build', 'gather', 'upgrade', 'train']\n",
    "ACTION_INDEX = {name: idx for idx, name in enumerate(ACTION_LIST)}\n",
    "\n",
    "SCREEN_SIZE = 84\n",
    "\n",
    "# ─── Terran Unit Types ───────────────────────────────────────────────────────\n",
    "TERRAN_STRUCTURE_TYPES = [\n",
    "    18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30,\n",
    "    130, 131, 132, 133\n",
    "]\n",
    "\n",
    "# ─── Enemy Tracking for Replay-Free Imitation ───────────────────────────────\n",
    "def extract_enemy_units(ts):\n",
    "    return [\n",
    "        (u.unit_type, u.x, u.y, u.health)\n",
    "        for u in ts.observation.feature_units\n",
    "        if u.alliance == features.PlayerRelative.ENEMY\n",
    "    ]\n",
    "\n",
    "def infer_enemy_action(prev_units, curr_units):\n",
    "    if not prev_units or not curr_units:\n",
    "        return \"idle\"\n",
    "\n",
    "    for (ptype, x0, y0, hp0), (ptype2, x1, y1, hp1) in zip(prev_units, curr_units):\n",
    "        if ptype != ptype2:\n",
    "            continue\n",
    "        if x0 != x1 or y0 != y1:\n",
    "            return \"move\"\n",
    "        elif hp1 < hp0:\n",
    "            return \"attack\"\n",
    "    return \"idle\"\n",
    "\n",
    "# ─── Observation Preprocessing ───────────────────────────────────────────────\n",
    "def safe_coords(x, y, screen_size=SCREEN_SIZE):\n",
    "    x = max(0, min(screen_size - 1, x))\n",
    "    y = max(0, min(screen_size - 1, y))\n",
    "    return [x, y]\n",
    "\n",
    "def preprocess(ts):\n",
    "    fs = ts.observation.feature_screen\n",
    "    pr = fs[_PLAYER_RELATIVE].astype(np.float32) / 4.0\n",
    "    ut = fs[_UNIT_TYPE].astype(np.float32) / fs[_UNIT_TYPE].max()\n",
    "    stacked = np.stack([pr, ut], axis=0)\n",
    "    return torch.from_numpy(stacked).unsqueeze(0).float()\n",
    "\n",
    "# ─── Legal Action Filter ─────────────────────────────────────────────────────\n",
    "def legal_actions(ts):\n",
    "    avail = set(ts.observation.available_actions)\n",
    "    fus = ts.observation.feature_units\n",
    "    legal = [ACTION_INDEX['do_nothing']]\n",
    "\n",
    "    if actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        legal.append(ACTION_INDEX['move'])\n",
    "    if actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        legal.append(ACTION_INDEX['attack'])\n",
    "    if any('Build' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['build'])\n",
    "    if actions.FUNCTIONS.Harvest_Gather_screen.id in avail and any(u.unit_type == 341 for u in fus):\n",
    "        legal.append(ACTION_INDEX['gather'])\n",
    "    if any('Research' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['upgrade'])\n",
    "    if any('Train' in actions.FUNCTIONS[a].name for a in avail):\n",
    "        legal.append(ACTION_INDEX['train'])\n",
    "\n",
    "    return legal\n",
    "\n",
    "# ─── PySC2 Action Execution Wrapper ─────────────────────────────────────────\n",
    "def make_pysc2_call(action_idx, ts, pending=None):\n",
    "    obs = ts.observation\n",
    "    fus = obs.feature_units\n",
    "    avail = set(obs.available_actions)\n",
    "\n",
    "    if pending:\n",
    "        if pending['action_fn'] in avail:\n",
    "            args = pending['args']\n",
    "            if len(args) > 1 and isinstance(args[1], list) and len(args[1]) == 2:\n",
    "                x, y = args[1]\n",
    "                return actions.FunctionCall(pending['action_fn'], [args[0], safe_coords(x, y)]), None\n",
    "            else:\n",
    "                return actions.FunctionCall(pending['action_fn'], args), None\n",
    "        else:\n",
    "            print(f\"[SKIP] Function {pending['action_fn']} not available anymore.\")\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['train']:\n",
    "        building_units = [u for u in fus if u.alliance == features.PlayerRelative.SELF and u.unit_type in TERRAN_STRUCTURE_TYPES]\n",
    "        if not building_units or actions.FUNCTIONS.select_point.id not in avail:\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "        building = random.choice(building_units)\n",
    "        select_coords = safe_coords(building.x, building.y)\n",
    "        select_action = actions.FunctionCall(actions.FUNCTIONS.select_point.id, [[0], select_coords])\n",
    "\n",
    "        train_actions = [a for a in avail if 'Train' in actions.FUNCTIONS[a].name]\n",
    "        if train_actions:\n",
    "            train_action = random.choice(train_actions)\n",
    "            next_action = {'action_fn': train_action, 'args': [[0]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "        return select_action, next_action\n",
    "\n",
    "    selectable_units = [u for u in fus if u.alliance == features.PlayerRelative.SELF]\n",
    "    if not selectable_units or actions.FUNCTIONS.select_point.id not in avail:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    unit = random.choice(selectable_units)\n",
    "    select_coords = safe_coords(unit.x, unit.y)\n",
    "    select_action = actions.FunctionCall(actions.FUNCTIONS.select_point.id, [[0], select_coords])\n",
    "\n",
    "    if action_idx == ACTION_INDEX['move'] and actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        x, y = np.random.randint(0, SCREEN_SIZE), np.random.randint(0, SCREEN_SIZE)\n",
    "        next_action = {'action_fn': actions.FUNCTIONS.Move_screen.id, 'args': [[0], [x, y]]}\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['attack'] and actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        enemies = [u for u in fus if u.alliance == features.PlayerRelative.ENEMY]\n",
    "        if enemies:\n",
    "            target = random.choice(enemies)\n",
    "            next_action = {'action_fn': actions.FUNCTIONS.Attack_screen.id, 'args': [[0], [target.x, target.y]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['gather'] and actions.FUNCTIONS.Harvest_Gather_screen.id in avail:\n",
    "        minerals = [u for u in fus if u.unit_type == 341]\n",
    "        if minerals:\n",
    "            target = random.choice(minerals)\n",
    "            next_action = {'action_fn': actions.FUNCTIONS.Harvest_Gather_screen.id, 'args': [[0], [target.x, target.y]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['build']:\n",
    "        build_actions = [a for a in avail if 'Build' in actions.FUNCTIONS[a].name]\n",
    "        if build_actions:\n",
    "            build_action = random.choice(build_actions)\n",
    "            buildable = np.argwhere(obs.feature_screen.buildable == 1)\n",
    "            if buildable.size > 0:\n",
    "                y, x = random.choice(buildable)\n",
    "                next_action = {'action_fn': build_action, 'args': [[0], [x, y]]}\n",
    "            else:\n",
    "                next_action = None\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    elif action_idx == ACTION_INDEX['upgrade']:\n",
    "        upgrade_actions = [a for a in avail if 'Research' in actions.FUNCTIONS[a].name]\n",
    "        if upgrade_actions:\n",
    "            upgrade_action = random.choice(upgrade_actions)\n",
    "            next_action = {'action_fn': upgrade_action, 'args': [[0]]}\n",
    "        else:\n",
    "            next_action = None\n",
    "\n",
    "    else:\n",
    "        next_action = None\n",
    "\n",
    "    return select_action, next_action\n",
    "\n",
    "# ─── Core Call (No Select Chain) ─────────────────────────────────────────────\n",
    "def make_pysc2_call_core(action_idx, ts):\n",
    "    obs = ts.observation\n",
    "    fus = obs.feature_units\n",
    "    avail = set(obs.available_actions)\n",
    "\n",
    "    if action_idx == ACTION_INDEX['do_nothing']:\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['move'] and actions.FUNCTIONS.Move_screen.id in avail:\n",
    "        x, y = np.random.randint(0, SCREEN_SIZE), np.random.randint(0, SCREEN_SIZE)\n",
    "        return actions.FunctionCall(actions.FUNCTIONS.Move_screen.id, [[0], safe_coords(x, y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['attack'] and actions.FUNCTIONS.Attack_screen.id in avail:\n",
    "        enemies = [u for u in fus if u.alliance == features.PlayerRelative.ENEMY]\n",
    "        if enemies:\n",
    "            target = random.choice(enemies)\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.Attack_screen.id, [[0], safe_coords(target.x, target.y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['build']:\n",
    "        build_actions = [a for a in avail if 'Build' in actions.FUNCTIONS[a].name]\n",
    "        if build_actions:\n",
    "            build_action = random.choice(build_actions)\n",
    "            buildable = np.argwhere(obs.feature_screen.buildable == 1)\n",
    "            if buildable.size > 0:\n",
    "                y, x = random.choice(buildable)\n",
    "                return actions.FunctionCall(build_action, [[0], safe_coords(x, y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['gather'] and actions.FUNCTIONS.Harvest_Gather_screen.id in avail:\n",
    "        minerals = [u for u in fus if u.unit_type == 341]\n",
    "        if minerals:\n",
    "            target = random.choice(minerals)\n",
    "            return actions.FunctionCall(actions.FUNCTIONS.Harvest_Gather_screen.id, [[0], safe_coords(target.x, target.y)]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['upgrade']:\n",
    "        upgrade_actions = [a for a in avail if 'Research' in actions.FUNCTIONS[a].name]\n",
    "        if upgrade_actions:\n",
    "            upgrade_action = random.choice(upgrade_actions)\n",
    "            return actions.FunctionCall(upgrade_action, [[0]]), None\n",
    "\n",
    "    if action_idx == ACTION_INDEX['train']:\n",
    "        train_actions = [a for a in avail if 'Train' in actions.FUNCTIONS[a].name]\n",
    "        if train_actions:\n",
    "            train_action = random.choice(train_actions)\n",
    "            return actions.FunctionCall(train_action, [[0]]), None\n",
    "\n",
    "    return actions.FunctionCall(actions.FUNCTIONS.no_op.id, []), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d4b4d6-c7fb-47f8-9b29-89ca20299c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO training LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f710a40-241f-4cef-91a8-4413a650d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from pysc2.lib import actions\n",
    "from config import *\n",
    "# from utils import preprocess, legal_actions, make_pysc2_call, extract_enemy_units, infer_enemy_action\n",
    "\n",
    "def PPO(envs, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1.0, end_factor=0.0, total_iters=MAX_ITERS\n",
    "    )\n",
    "\n",
    "    ep_rewards = []\n",
    "    expert_dataset = []  # Collect enemy bot data\n",
    "\n",
    "    logger.info(\"▶️  Starting PPO for %d iterations\", MAX_ITERS)\n",
    "    for it in range(MAX_ITERS):\n",
    "        if it % 1000 == 0:\n",
    "            logger.info(\"🔄 Iter %d / %d\", it, MAX_ITERS)\n",
    "\n",
    "        # storage buffers\n",
    "        obs_buf  = torch.zeros(envs.nb, T, 2, SCREEN_SIZE, SCREEN_SIZE, device=DEVICE)\n",
    "        act_buf  = torch.zeros(envs.nb, T,      dtype=torch.long, device=DEVICE)\n",
    "        logp_buf = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        val_buf  = torch.zeros(envs.nb, T+1,                   device=DEVICE)\n",
    "        rew_buf  = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        done_buf = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "        adv_buf  = torch.zeros(envs.nb, T,                     device=DEVICE)\n",
    "\n",
    "        # ─── Rollout ─────────────────────────────────────────────────────────\n",
    "        with torch.no_grad():\n",
    "            for t in range(T):\n",
    "                for i in range(envs.nb):\n",
    "                    ts    = envs.obs[i]\n",
    "                    state = preprocess(ts)\n",
    "                    logits, value = model(state)\n",
    "\n",
    "                    # mask illegal\n",
    "                    LA   = legal_actions(ts)\n",
    "                    mask = torch.full_like(logits, float('-inf'))\n",
    "                    mask[0, LA] = 0.0\n",
    "                    dist = Categorical(logits=logits + mask)\n",
    "\n",
    "                    action = dist.sample()\n",
    "                    logp   = dist.log_prob(action)\n",
    "                    fc     = make_pysc2_call(action.item(), ts)\n",
    "\n",
    "                    # step (fallback to no-op)\n",
    "                    try:\n",
    "                        ts2 = envs.step(i, fc)\n",
    "                    except ValueError:\n",
    "                        ts2 = envs.step(i, actions.FunctionCall(actions.FUNCTIONS.no_op.id, []))\n",
    "\n",
    "                    r = ts2.reward\n",
    "                    d = float(ts2.last())\n",
    "\n",
    "                    obs_buf[i,t]  = state\n",
    "                    act_buf[i,t]  = action\n",
    "                    logp_buf[i,t] = logp\n",
    "                    val_buf[i,t]  = value\n",
    "                    rew_buf[i,t]  = r\n",
    "                    done_buf[i,t] = d\n",
    "\n",
    "                    if d:\n",
    "                        ep_rewards.append(sum(rew_buf[i, :t+1].tolist()))\n",
    "                        envs.reset(i)\n",
    "\n",
    "            for i in range(envs.nb):\n",
    "                val_buf[i,T] = model(preprocess(envs.obs[i]))[1]\n",
    "\n",
    "        # ─── GAE & flatten ────────────────────────────────────────────────────\n",
    "        for i in range(envs.nb):\n",
    "            gae = 0\n",
    "            for t in reversed(range(T)):\n",
    "                mask  = 1.0 - done_buf[i,t]\n",
    "                delta = rew_buf[i,t] + GAMMA*val_buf[i,t+1]*mask - val_buf[i,t]\n",
    "                gae   = delta + GAMMA*GAE_LAMBDA*mask*gae\n",
    "                adv_buf[i,t] = gae\n",
    "\n",
    "        b_s  = obs_buf.reshape(-1,2,SCREEN_SIZE,SCREEN_SIZE)\n",
    "        b_a  = act_buf.reshape(-1)\n",
    "        b_lp = logp_buf.reshape(-1)\n",
    "        b_v  = val_buf[:,:T].reshape(-1)\n",
    "        b_ad = adv_buf.reshape(-1)\n",
    "\n",
    "        # ─── PPO updates ─────────────────────────────────────────────────────\n",
    "        for _ in range(K):\n",
    "            ds     = TensorDataset(b_s,b_a,b_lp,b_v,b_ad)\n",
    "            loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            for st, ac, old_lp, old_v, adv in loader:\n",
    "                logits, val = model(st)\n",
    "                dist        = Categorical(logits=logits)\n",
    "                lp          = dist.log_prob(ac)\n",
    "                ratio       = torch.exp(lp - old_lp)\n",
    "\n",
    "                clip   = 0.1 * (1 - it/MAX_ITERS)\n",
    "                obj1   = adv * ratio\n",
    "                obj2   = adv * torch.clamp(ratio, 1-clip, 1+clip)\n",
    "                p_loss = -torch.min(obj1,obj2).mean()\n",
    "\n",
    "                ret     = adv + old_v\n",
    "                v1      = (val - ret).pow(2)\n",
    "                v2      = (torch.clamp(val,old_v-clip,old_v+clip)-ret).pow(2)\n",
    "                v_loss  = 0.5 * torch.max(v1,v2).mean()\n",
    "\n",
    "                entropy = dist.entropy().mean()\n",
    "                loss    = p_loss + VF_COEF*v_loss - ENT_COEF*entropy\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(),0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # ─── Save replays only at end ────────────────────────────────────────────\n",
    "    for i in range(envs.nb):\n",
    "        replay_path = os.path.join(REPLAY_DIR, f\"ppo_final_{i}.SC2Replay\")\n",
    "        envs.envs[i]._save_replay(\"PPO\", replay_path)\n",
    "    logger.info(\"💾 Saved final replay(s) to %s\", REPLAY_DIR)\n",
    "\n",
    "    # ─── Plot learning curve ─────────────────────────────────────────────────\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(ep_rewards, label=\"episode reward\")\n",
    "    plt.title(\"Environment Reward per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"learning_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "    envs.close()\n",
    "    logger.info(\"✅ Training complete\")\n",
    "    logger.info(f\"Saved learning_curve.png over {len(ep_rewards)} episodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdcecead-f59c-4bbd-827d-771ad655c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47d18d4c-cd99-477a-be83-74fd6005978b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from absl import app\n",
    "# # from environment import SC2Envs\n",
    "# # from model import ActorCritic\n",
    "# # from ppo import PPO\n",
    "# # from config import NB_ACTORS, DEVICE\n",
    "# # from utils import ACTION_LIST\n",
    "\n",
    "# def main(_):\n",
    "#     envs = SC2Envs(NB_ACTORS)\n",
    "#     model = ActorCritic(2, len(ACTION_LIST)).to(DEVICE)\n",
    "#     PPO(envs, model)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#     sys.argv = sys.argv[:1]  # Remove extra flags passed by Jupyter or IPython\n",
    "#     app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "942d6802-c091-4439-9f6c-a28b16ccbb30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from rich.live import Live\n",
    "# from rich.table import Table\n",
    "# from rich.console import Console\n",
    "# from collections import deque\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import sys\n",
    "# from absl import flags\n",
    "\n",
    "# flags.FLAGS(sys.argv)  # fix required by pysc2\n",
    "# # from util import preprocess, legal_actions, make_pysc2_call\n",
    "# # from env import SC2Envs\n",
    "\n",
    "# console = Console()\n",
    "# envs = SC2Envs(nb_actor=1)\n",
    "# pending_action = [None] * envs.nb\n",
    "\n",
    "# MAX_ROWS = 20\n",
    "# recent_rows = deque(maxlen=MAX_ROWS)\n",
    "\n",
    "# # For tracking per-episode scores\n",
    "# episode_score = [0] * envs.nb\n",
    "# scores = []\n",
    "\n",
    "# def generate_table():\n",
    "#     table = Table(title=f\"SC2 Agent Actions (Last {MAX_ROWS} Steps)\", expand=True)\n",
    "#     table.add_column(\"Step\", justify=\"right\")\n",
    "#     table.add_column(\"Function ID\", justify=\"right\")\n",
    "#     table.add_column(\"Args\", justify=\"left\")\n",
    "#     for row in recent_rows:\n",
    "#         table.add_row(*row)\n",
    "#     return table\n",
    "\n",
    "# with Live(generate_table(), refresh_per_second=10, console=console, transient=True) as live:\n",
    "#     for step in range(MAX_ITERS):\n",
    "#         for i in range(envs.nb):\n",
    "#             ts = envs.obs[i]\n",
    "\n",
    "#             if pending_action[i]:\n",
    "#                 action, pending_action[i] = make_pysc2_call(None, ts, pending_action[i])\n",
    "#             else:\n",
    "#                 legal = legal_actions(ts)\n",
    "#                 action_idx = random.choice(legal)\n",
    "#                 action, pending_action[i] = make_pysc2_call(action_idx, ts)\n",
    "\n",
    "#             recent_rows.append((str(step), str(action.function), str(action.arguments)))\n",
    "#             live.update(generate_table())\n",
    "\n",
    "#             ts = envs.step(i, action)\n",
    "#             episode_score[i] += ts.reward\n",
    "\n",
    "#             if ts.last():\n",
    "#                 scores.append(episode_score[i])\n",
    "#                 episode_score[i] = 0  # reset\n",
    "#                 envs.reset(i)\n",
    "\n",
    "# envs.close()\n",
    "\n",
    "# # Plot episode scores\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(scores, label=\"Episode Score\", marker='o', linewidth=1.5)\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Total Score\")\n",
    "# plt.title(\"Agent Score per Episode\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1044ec-ea4c-4ed4-91c4-6cede1bb862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Campain Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b570e-8f16-4ebf-ba44-aef48488f488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                         SC2 Agent Actions (Last 20 Steps)                                         </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">              Step </span>┃<span style=\"font-weight: bold\">                            Function ID </span>┃<span style=\"font-weight: bold\"> Args                                               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│               374 │                                    331 │ [[0], [59, 69]]                                    │\n",
       "│               374 │                                      2 │ [[0], ]                                            │\n",
       "│               375 │                                      0 │ []                                                 │\n",
       "│               375 │                                    264 │ [[0], ]                                            │\n",
       "│               375 │                                      2 │ [[0], ]                                            │\n",
       "│               375 │                                      2 │ [[0], ]                                            │\n",
       "│               375 │                                      0 │ []                                                 │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               376 │                                      0 │ []                                                 │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               378 │                                      2 │ [[0], ]                                            │\n",
       "│               378 │                                      2 │ [[0], ]                                            │\n",
       "│               378 │                                      2 │ [[0], ]                                            │\n",
       "└───────────────────┴────────────────────────────────────────┴────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                         SC2 Agent Actions (Last 20 Steps)                                         \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m             Step\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                           Function ID\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mArgs                                              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│               374 │                                    331 │ [[0], [59, 69]]                                    │\n",
       "│               374 │                                      2 │ [[0], ]                                            │\n",
       "│               375 │                                      0 │ []                                                 │\n",
       "│               375 │                                    264 │ [[0], ]                                            │\n",
       "│               375 │                                      2 │ [[0], ]                                            │\n",
       "│               375 │                                      2 │ [[0], ]                                            │\n",
       "│               375 │                                      0 │ []                                                 │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               376 │                                      0 │ []                                                 │\n",
       "│               376 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               377 │                                      2 │ [[0], ]                                            │\n",
       "│               378 │                                      2 │ [[0], ]                                            │\n",
       "│               378 │                                      2 │ [[0], ]                                            │\n",
       "│               378 │                                      2 │ [[0], ]                                            │\n",
       "└───────────────────┴────────────────────────────────────────┴────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "import matplotlib.pyplot as plt\n",
    "from absl import flags\n",
    "\n",
    "# from environment import SC2EnvsMulti\n",
    "# from utils import make_pysc2_call, legal_actions  # your existing helpers\n",
    "\n",
    "# Fix for absl.flags in script context\n",
    "flags.FLAGS(sys.argv, known_only=True)\n",
    "\n",
    "# ─── Settings ────────────────────────────────────────────────────────────────\n",
    "NUM_EPISODES  = 20\n",
    "NB_ACTORS     = 5\n",
    "REPLAY_DIR    = \"replays\"\n",
    "REPLAY_PREFIX = \"pysc2_run\"\n",
    "\n",
    "console = Console()\n",
    "envs = SC2EnvsMulti(\n",
    "    nb_actor=NB_ACTORS,\n",
    "    replay_dir=REPLAY_DIR,\n",
    "    replay_prefix=REPLAY_PREFIX,\n",
    ")\n",
    "\n",
    "pending_action = [None] * envs.nb\n",
    "MAX_ROWS    = 20\n",
    "recent_rows = deque(maxlen=MAX_ROWS)\n",
    "scores      = []\n",
    "\n",
    "def generate_table():\n",
    "    table = Table(title=f\"SC2 Agent Actions (Last {MAX_ROWS} Steps)\", expand=True)\n",
    "    table.add_column(\"Step\", justify=\"right\")\n",
    "    table.add_column(\"Function ID\", justify=\"right\")\n",
    "    table.add_column(\"Args\", justify=\"left\")\n",
    "    for row in recent_rows:\n",
    "        table.add_row(*row)\n",
    "    return table\n",
    "\n",
    "# ─── Episode Loop ─────────────────────────────────────────────────────────────\n",
    "with Live(generate_table(), refresh_per_second=10, console=console, transient=True) as live:\n",
    "    for ep in range(1, NUM_EPISODES + 1):\n",
    "        # reset each parallel env\n",
    "        for i in range(envs.nb):\n",
    "            envs.reset(i)\n",
    "        episode_score = [0] * envs.nb\n",
    "\n",
    "        console.log(f\"[blue]=== Episode {ep} ===[/blue]\")\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            for i in range(envs.nb):\n",
    "                ts = envs.obs[i]\n",
    "\n",
    "                if pending_action[i]:\n",
    "                    action, pending_action[i] = make_pysc2_call(None, ts, pending_action[i])\n",
    "                else:\n",
    "                    legal  = legal_actions(ts)\n",
    "                    choice = random.choice(legal)\n",
    "                    action, pending_action[i] = make_pysc2_call(choice, ts)\n",
    "\n",
    "                recent_rows.append((str(step), str(action.function), str(action.arguments)))\n",
    "                live.update(generate_table())\n",
    "\n",
    "                ts = envs.step(i, action)\n",
    "                episode_score[i] += ts.reward\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            if any(envs.done):\n",
    "                console.log(f\"[green]Episode {ep} done, score: {episode_score}[/green]\")\n",
    "                scores.extend(episode_score)\n",
    "                break\n",
    "\n",
    "# ─── Cleanup ────────────────────────────────────────────────────────────────\n",
    "envs.close()\n",
    "\n",
    "# ─── Plot Episode Scores ────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(scores, label=\"Episode Score\", marker=\"o\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Score\")\n",
    "plt.title(\"Agent Score per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87a1e3-1c57-41da-a461-9c6a4a0b6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features\n",
    "\n",
    "def main():\n",
    "    # Make an absolute folder INSIDE YOUR PROJECT\n",
    "    PROJECT_REPLAY_DIR = os.path.join(os.getcwd(), \"replays\")\n",
    "    os.makedirs(PROJECT_REPLAY_DIR, exist_ok=True)\n",
    "\n",
    "    with sc2_env.SC2Env(\n",
    "        map_name=\"Simple64\",\n",
    "        players=[\n",
    "            sc2_env.Agent(sc2_env.Race.terran),\n",
    "            sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty.very_easy),\n",
    "        ],\n",
    "        agent_interface_format=features.AgentInterfaceFormat(\n",
    "            feature_dimensions=features.Dimensions(screen=84, minimap=64),\n",
    "            use_feature_units=True,\n",
    "        ),\n",
    "        step_mul=8,\n",
    "        visualize=False,\n",
    "\n",
    "        save_replay_episodes=1,\n",
    "        # ← NOW AN ABSOLUTE PATH\n",
    "        replay_dir=PROJECT_REPLAY_DIR,\n",
    "        replay_prefix=\"pysc2_run\",\n",
    "    ) as env:\n",
    "        for ep in range(1, 2):\n",
    "            print(f\"=== Episode {ep} ===\")\n",
    "            timesteps = env.reset()\n",
    "            total_reward = 0.0\n",
    "            while True:\n",
    "                # no‐op is always valid\n",
    "                action = actions.FUNCTIONS.no_op()\n",
    "                timesteps = env.step([action])\n",
    "                total_reward += timesteps[0].reward\n",
    "                if timesteps[0].last():\n",
    "                    print(f\"Episode {ep} ended, reward = {total_reward}\")\n",
    "                    break\n",
    "\n",
    "    print(f\"Replays are in {PROJECT_REPLAY_DIR}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204dc52-7157-4d41-9c48-52bba64e0baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202095a-e051-4db0-8711-0b827819118a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
